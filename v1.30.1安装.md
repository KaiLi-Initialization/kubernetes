# 环境初始化

## 1 检查操作系统的版本

```powershell
# 此方式下安装kubernetes集群系统版本为：CentOS Linux release 7.9.2009 (Core) 版本号：4.18.0-408.el8.x86_64
[root@CentOS-K8S-Master ~]# cat /etc/redhat-release 
CentOS Linux release 7.9.2009 (Core)
[root@centos-master ~]# uname -a
Linux centos-master 3.10.0-1160.95.1.el7.x86_64 #1 SMP Mon Jul 24 13:59:37 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux


```

## 2 主机名解析

为了方便集群节点间的直接调用，在这个配置一下主机名解析，企业中推荐使用内部DNS服务器

```powershell
修改主机名使用：hostnamectl set-hostname 主机名 

# 主机名成解析 编辑三台服务器的/etc/hosts文件，添加下面内容

cat >> /etc/hosts <<EOF
192.168.10.131  k8s-master01
192.168.10.132  k8s-master02
192.168.10.141  k8s-node01
192.168.10.142  k8s-node02
EOF
```

## 3 时间同步

kubernetes要求集群中的节点时间必须精确一直，这里使用chronyd服务从网络同步时间

企业中建议配置内部的会见同步服务器

```powershell
# 启动chronyd服务
[root@master ~]# systemctl start chronyd
[root@master ~]# systemctl enable chronyd
[root@master ~]# date
```

## 4  禁用iptable和firewalld服务

kubernetes和docker 在运行的中会产生大量的iptables规则，为了不让系统规则跟它们混淆，直接关闭系统的规则

```powershell
# 1 关闭firewalld服务
[root@master ~]# systemctl stop firewalld & systemctl disable firewalld

# 2 关闭iptables服务
[root@master ~]# systemctl stop iptables
[root@master ~]# systemctl disable iptables
```

## 5 禁用selinux

selinux是linux系统下的一个安全服务，如果不关闭它，在安装集群中会产生各种各样的奇葩问题

```powershell
# 编辑 /etc/selinux/config 文件，修改SELINUX的值为disable
# 注意修改完毕之后需要重启linux服务
SELINUX=disabled

命令方式修改
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
```

## 6 禁用swap分区

swap分区指的是虚拟内存分区，它的作用是物理内存使用完，之后将磁盘空间虚拟成内存来使用，启用swap设备会对系统的性能产生非常负面的影响，因此kubernetes要求每个节点都要禁用swap设备，但是如果因为某些原因确实不能关闭swap分区，就需要在集群安装过程中通过明确的参数进行配置说明

```powershell
方式一：重新启动电脑，永久禁用Swap
# 编辑分区配置文件/etc/fstab，注释掉swap分区一行
# 注意修改完毕之后需要重启linux服务
vim /etc/fstab
注释掉 /dev/mapper/centos-swap swap
# /dev/mapper/centos-swap swap

方式二：不重启电脑，禁用启用swap，立刻生效
swapoff -a 禁用命令
swapon -a  启用命令
free -mh   查看交换分区的状态:
```



## 7 修改linux的内核参数

管理员可以通过 sysctl 接口修改内核运行时的参数,在 `/proc/sys/` 虚拟文件系统下存放许多内核参数。这些参数涉及了多个内核子系统，如：

- 内核子系统（通常前缀为: `kernel.`）
- 网络子系统（通常前缀为: `net.`）
- 虚拟内存子系统（通常前缀为: `vm.`）

若要获取完整的参数列表，请执行以下命令：

```shell
sudo sysctl -a
```

* net.bridge.bridge-nf-call-iptables：开启桥设备内核监控（ipv4）
* net.ipv4.ip_forward：开启路由转发
* net.bridge.bridge-nf-call-ip6tables：开启桥设备内核监控（ipv6）



```shell
内核低于 4.1 版本需要添加 fs.may_detach_mounts=1 和 net.ipv4.tcp_tw_recycle=0

在内核低于 4.1 中，不要设置 net.ipv4.tcp_tw_recycle 这个参数为 1
，网上有不少教程没提到或者内核版本过低系统默认设置为 1
开启此参数，对于外网的 sockets 链接会快速回收。但是对于内网会导致大量的 TCP 链接建立错误。k8s
使用的都是在内网，所以要禁用！设置为 0
有关 net.ipv4.tcp_tw_recycle 参数查看文章(https://cloud.tencent.com/developer/article/1683704)

参数 fs.may_detach_mounts 则是跟容器相关的。该参数如果设置为 0，会导致服务变更后旧 pod
在回收时会一直卡在 Terminating 的状态，会重复出现 UnmountVolume.TearDown failed for volume 错误。
有关 fs.may_detach_mounts 参数产生的 bug 查看文章(https://github.com/kubernetes/kubernetes/issues/51835)
以及 (https://bugzilla.redhat.com/show_bug.cgi?id=1441737) 。
```

以上3项为必须参数，其他参数可根据需要添加

```powershell
# 修改linux的内核采纳数，添加网桥过滤和地址转发功能
# 编辑/etc/sysctl.d/kubernetes.conf文件，添加如下配置：

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# 设置所需的 sysctl 参数，参数在重新启动后保持不变

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# 应用 sysctl 参数而不重新启动
sudo sysctl --system

# 重新加载配置
[root@master ~]# sysctl -p
# 加载网桥过滤模块
[root@master ~]# modprobe br_netfilter & modprobe overlay

# 查看网桥过滤模块是否加载成功
[root@master ~]# lsmod | grep br_netfilter & lsmod | grep overlay

```

通过运行以下指令确认 `net.bridge.bridge-nf-call-iptables`、`net.bridge.bridge-nf-call-ip6tables` 和 `net.ipv4.ip_forward` 系统变量在你的 `sysctl` 配置中被设置为 1：

```bash
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward
```

## 8 配置ipvs功能

在Kubernetes中Service有两种带来模型，一种是基于iptables的，一种是基于ipvs的两者比较的话，ipvs的性能明显要高一些，但是如果要使用它，需要手动载入ipvs模块

参考文档：https://kubernetes.io/zh-cn/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/

服务代理：https://kubernetes.io/zh-cn/docs/reference/networking/virtual-ips/



IPVS (IP Virtual Server)用于实现传输层负载均衡。

```powershell
# 1.安装ipset和ipvsadm
[root@master ~]# yum install ipset ipvsadm -y
# 2.添加需要加载的模块写入脚本文件
[root@master ~]# cat <<EOF> /etc/sysconfig/modules/ipvs.modules
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

# 在内核4.19+版本nf_conntrack_ipv4已经改为nf_conntrack， 4.18以下使用nf_conntrack_ipv4即可
[root@master ~]# cat <<EOF> /etc/sysconfig/modules/ipvs.modules
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
EOF


# 3.为脚本添加执行权限
[root@master ~]# chmod +x /etc/sysconfig/modules/ipvs.modules
# 4.执行脚本文件
[root@master ~]# /bin/bash /etc/sysconfig/modules/ipvs.modules
# 5.查看对应的模块是否加载成功
[root@master ~]# lsmod | grep -e ip_vs -e nf_conntrack
低版本运行 ：
[root@master ~]# lsmod | grep -e ip_vs -e nf_conntrack_ipv4
```

## 9 设置系统时区与同步

设置时区

```shell
timedatectl set-timezone Asia/ShanghaiCOPY
```

同步时区

```shell
systemctl enable chronyd && \
systemctl start chronyd
```

查看

```shell
timedatectl status
```

*显示如下*

>`Time zone: Asia/Shanghai (CST, +0800)` 表示使用东八区时区
>`System clock synchronized: yes` 表示时区有同步
>`NTP service: active` 表示开启了时区同步服务

```text
               Local time: 三 2022-09-21 01:12:09 CST
           Universal time: 二 2022-09-20 17:12:09 UTC
                 RTC time: 二 2022-09-20 17:12:09
                Time zone: Asia/Shanghai (CST, +0800)
System clock synchronized: yes
              NTP service: active
          RTC in local TZ: no
```

将当前的 UTC 时间写入硬件时钟

```shell
timedatectl set-local-rtc 0
```

重启依赖于系统时间的服务

```shell
systemctl restart rsyslog && \
systemctl restart crond
```





# 二进制安装containerd

containerd已包含Kubernetes CRI 功能，和docker不同的是无需下载`cri-containerd-....`档案即可使用 CRI。

二进制安装containerd需要**`runc`**和**`CNI`**插件的支持。

#### 第 1 步：安装containerd

参考文档：https://github.com/containerd/containerd/blob/main/docs/getting-started.md

下载地址：https://containerd.io/downloads/

所有设备安装（`master`和`node`节点）

containerd 的官方二次制作版本可用于`amd64`（也称为`x86_64`）和`arm64`（也称为`aarch64`）结构。

[从https://github.com/containerd/containerd/releases](https://github.com/containerd/containerd/releases)下载`containerd-<VERSION>-<OS>-<ARCH>.tar.gz`存档，验证其sha256sum，并将其解压到：`/usr/local`

```shell
wget https://github.com/containerd/containerd/releases/download/v1.7.14/containerd-1.7.14-linux-amd64.tar.gz

tar xvf containerd-1.7.14-linux-amd64.tar.gz

[root@master ~]# tar xvf containerd-1.7.14-linux-amd64.tar.gz -C /usr/local
bin/
bin/containerd-shim
bin/ctr
bin/containerd-shim-runc-v1
bin/containerd
bin/containerd-stress
bin/containerd-shim-runc-v2



# 文件说明
containerd 的安装包中一共有五个文件，通过上面的命令它们被安装到了 usr/local/bin 目录中：

1. containerd：即容器的运行时，以 gRPC 协议的形式提供满足 OCI 标准的 API

2. containerd-release：containerd 项目的发行版发布工具

3. containerd-stress：containerd压力测试工具

4. containerd-shim：这是每一个容器的运行时载体，我们在 docker 宿主机上看到的 shim 也正是代表着一个个通过调用 containerd 启动的 docker 容器。

5. ctr：它是一个简单的 CLI 接口，用作 containerd 本身的一些调试用途，投入生产使用时还是应该配合docker 或者 cri-containerd 部署。
```

##### 1）修改containerd配置

配置Containerd所需的模块（所有节点）

```shell
cat <<EOF> /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

# 所有节点加载模块：
modprobe -- overlay
modprobe -- br_netfilter
```

创建初始配置文件

Containerd 的默认配置文件为 `/etc/containerd/config.toml`

```shell
$ mkdir -p /etc/containerd/
$ containerd config default > /etc/containerd/config.toml    #创建默认的配置文件
```



##### 2）配置 `systemd` cgroup 驱动

**参考文档：**https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#docker

结合 `runc` 使用 `systemd` cgroup 驱动，在 `/etc/containerd/config.toml` 中设置：

```shell
vim /etc/containerd/config.toml

# 找到containerd.runtimes.runc.options，添加SystemdCgroup = true

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes]

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          base_runtime_spec = ""
          cni_conf_dir = ""
          cni_max_conf_num = 0
          container_annotations = []
          pod_annotations = []
          privileged_without_host_devices = false
          runtime_engine = ""
          runtime_path = ""
          runtime_root = ""
          runtime_type = "io.containerd.runc.v2"

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = "true"    # 此处修改为true
            SystemdCgroup = false
```

或者运行以下命令

```shell
sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml
```



**替换镜像源**

```shell
# 所有节点将sandbox_image的Pause镜像改成符合自己版本的地址：registry.aliyuncs.com/google_containers/pause:3.9
$ vim /etc/containerd/config.toml

  [plugins."io.containerd.grpc.v1.cri"]
    device_ownership_from_security_context = false
    disable_apparmor = false
    disable_cgroup = false
    disable_hugetlb_controller = true
    disable_proc_mount = false
    disable_tcp_service = true
    enable_selinux = false
    enable_tls_streaming = false
    enable_unprivileged_icmp = false
    enable_unprivileged_ports = false
    ignore_image_defined_volumes = false
    max_concurrent_downloads = 3
    max_container_log_line_size = 16384
    netns_mounts_under_state_dir = false
    restrict_oom_score_adj = false
    sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.9" # 此处更换
    
#等同于：
$ sed -i "s#registry.k8s.io/pause#registry.aliyuncs.com/google_containers/pause#g"  /etc/containerd/config.toml
```



##### 3）通过 systemd 启动 containerd

还应该从https://raw.githubusercontent.com/containerd/containerd/main/containerd.service下载 `containerd.service`文件到目录`/usr/local/lib/systemd/system/containerd.service`(目录/usr/local/lib/systemd/system/需要创建)或`/usr/lib/systemd/system/containerd.service`中，并运行以下命令：

```shell
wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service
mkdir -p /usr/local/lib/systemd/system/
cp containerd.service /usr/local/lib/systemd/system/
```

若无法下载可创建containerd.service将下文复制进去

```shell
vim /usr/lib/systemd/system/containerd.service

# Copyright The containerd Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target local-fs.target

[Service]
#uncomment to fallback to legacy CRI plugin implementation with podsandbox support.
#Environment="DISABLE_CRI_SANDBOXES=1"
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/containerd

Type=notify
Delegate=yes
KillMode=process
Restart=always
RestartSec=5
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity
LimitNOFILE=infinity
# Comment TasksMax if your systemd version does not supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
OOMScoreAdjust=-999

[Install]
WantedBy=multi-user.target
```

设置开机启动

```shell
systemctl daemon-reload
systemctl enable --now containerd
systemctl status containerd.service
containerd  --version    #查看版本
```



#### 第 2 步：安装 runc

[从https://github.com/opencontainers/runc/releases](https://github.com/opencontainers/runc/releases)下载`runc.<ARCH>`二进制文件，验证其 sha256sum，

并将其安装到目录.`/usr/local/sbin/runc`

此处安装版本为：`v1.1.4/runc.amd64`

```shell
[root@master ~]# wget https://github.com/opencontainers/runc/releases/download/v1.1.4/runc.amd64


[root@master ~]# install -m 755 runc.amd64 /usr/local/sbin/runc
#出现以下说明安装没有问题
[root@master ~]# runc -version
runc version 1.1.4
commit: v1.1.4-0-g5fd4c4d1
spec: 1.0.2-dev
go: go1.17.10
libseccomp: 2.5.4

```

该二进制文件是静态构建的，应该适用于任何 Linux 发行版。

#### 第 3 步：安装CNI插件

[从https://github.com/containernetworking/plugins/releases](https://github.com/containernetworking/plugins/releases)下载`cni-plugins-<OS>-<ARCH>-<VERSION>.tgz`存档，验证其 sha256sum，并将其解压到：`/opt/cni/bin`

```shell
[root@master ~]# wget https://github.com/containernetworking/plugins/releases/download/v1.5.0/cni-plugins-linux-amd64-v1.5.0.tgz
[root@master ~]# mkdir -p /opt/cni/bin
[root@master ~]# tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.5.0.tgz
./
./dhcp
./loopback
./README.md
./bandwidth
./ipvlan
./vlan
./static
./host-device
./LICENSE
./bridge
./dummy
./tuning
./vrf
./tap
./portmap
./firewall
./ptp
./host-local
./macvlan
./sbr

```





# 安装docker

1. 卸载旧版本

   ```shell
   sudo yum remove docker \
                     docker-client \
                     docker-client-latest \
                     docker-common \
                     docker-latest \
                     docker-latest-logrotate \
                     docker-logrotate \
                     docker-engine
   ```

   

3. 设置存储库

   ```shell
   sudo yum install -y yum-utils
   sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
   ```

4. 安装Docker Engine

   **方式一**：安装特定版本的docker-ce

   要安装特定版本，请首先列出存储库中的可用版本：

   ```shell
   yum list docker-ce --showduplicates | sort -r
   
   docker-ce.x86_64    3:25.0.0-1.el8    docker-ce-stable
   docker-ce.x86_64    3:24.0.7-1.el8    docker-ce-stable
   <...>
   ```

   

   必须制定--setopt=obsoletes=0，否则yum会自动安装更高版本

   ```shell
   sudo yum install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io docker-buildx-plugin docker-compose-plugin
   ```

   

   **方式二**：安装最新版本docker-ce

   ```shell
   sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
   ```

5. 使用 `systemd` 驱动

   **参考文档：**https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cgroupfs-cgroup-driver

    由于 kubeadm 把 kubelet 视为一个 [系统服务](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/kubelet-integration)来管理， 所以对基于 kubeadm 的安装， 我们推荐使用 `systemd` 驱动， 不推荐 kubelet [默认](https://kubernetes.io/zh-cn/docs/reference/config-api/kubelet-config.v1beta1)的 `cgroupfs` 驱动。

   

   Docker 在默认情况下使用Vgroup Driver为cgroupfs，而Kubernetes推荐使用systemd来替代cgroupfs

   

   ```shell
   mkdir -p /etc/docker
   
   cat <<EOF> /etc/docker/daemon.json
   {
   	"exec-opts": ["native.cgroupdriver=systemd"]      
   }
   EOF
   ```

6. 启动dokcer

   ```shell
   systemctl start docker & systemctl enable docker
   ```

   

## 卸载 Docker Engine



1. 卸载 Docker Engine、CLI、containerd 和 Docker Compose 软件包：

   ```console
   sudo yum remove docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras
   ```

2. 主机上的镜像、容器、卷或自定义配置文件不会自动删除。要删除所有镜像、容器和卷，请执行以下操作：

   

   ```console
   sudo rm -rf /var/lib/docker
   sudo rm -rf /var/lib/containerd
   ```

您必须手动删除任何已编辑的配置文件。



##  cri-dockerd安装

kubernetes1.26.3版本需要安装cri-dockerd作为网络接口

**参考文档：**https://github.com/mirantis/cri-dockerd#build-and-install

[cri-dockerd下载地址](https://github.com/Mirantis/cri-dockerd/releases)

###### 3.6.11.1 rpm方式安装cri-dockerd

下载rpm包

```shell
yum install -y wget
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.1/cri-dockerd-0.3.1-3.el7.x86_64.rpm
```

安装cri-dockerd

```shell
yum install -y cri-dockerd-0.3.1-3.el7.x86_64.rpm

systemctl daemon-reload
systemctl start cri-docker
systemctl enable cri-docker.service
systemctl enable --now cri-docker.socket

systemctl status cri-docker
```

###### 3.6.11.2 go编译安装cri-dockerd

**安装go环境**

```shell
wget https://storage.googleapis.com/golang/getgo/installer_linux
chmod +x ./installer_linux
./installer_linux
source ~/.bash_profile
```

**克隆cri-dockerd文件**

```shell
git clone https://github.com/Mirantis/cri-dockerd.git
```

**编译安装cri-dockerd**

```shell
cd cri-dockerd
mkdir bin
go build -o bin/cri-dockerd
mkdir -p /usr/local/bin
install -o root -g root -m 0755 bin/cri-dockerd /usr/local/bin/cri-dockerd
cp -a packaging/systemd/* /etc/systemd/system
sed -i -e 's,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,' /etc/systemd/system/cri-docker.service
systemctl daemon-reload
systemctl enable cri-docker.service
systemctl enable --now cri-docker.socket
```

**Kubernetes使用**（安装组件后配置）

1. Kubernetes网络插件默认是cni，需要追加**`--network-plugin=cni`**，通过该配置告诉容器，使用kubernetes的网络接口。
2. pause镜像是一切的 Pod 的基础

```shell
# go编译安装路径
vim /etc/systemd/system/cri-docker.service
# rpm安装路径
vim /usr/lib/systemd/system/cri-docker.service

[root@master ~]# cat /etc/systemd/system/cri-docker.service 
[Unit]
Description=CRI Interface for Docker Application Container Engine
Documentation=https://docs.mirantis.com
After=network-online.target firewalld.service docker.service
Wants=network-online.target
Requires=cri-docker.socket

[Service]
Type=notify
ExecStart=/usr/local/bin/cri-dockerd --container-runtime-endpoint fd:// # 此处添加一下内容
ExecReload=/bin/kill -s HUP $MAINPID
TimeoutSec=0
RestartSec=2
Restart=always

# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
# Both the old, and new location are accepted by systemd 229 and up, so using the old location
# to make them work for either version of systemd.
StartLimitBurst=3

# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
# this option work for either version of systemd.
StartLimitInterval=60s

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Comment TasksMax if your systemd version does not support it.
# Only systemd 226 and above support this option.
TasksMax=infinity
Delegate=yes
KillMode=process

[Install]
WantedBy=multi-user.target

```

需要将以上两条1，2的参数追加在ExecStart后面

```shell
ExecStart=/usr/local/bin/cri-dockerd --container-runtime-endpoint fd:// --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9
```

**启动cri-dockerd**

```shell
systemctl daemon-reload
systemctl enable cri-docker.service
systemctl enable --now cri-docker.socket
systemctl start cri-docker
```

验证cri-dockerd

```shell
[root@master ~]# systemctl status cri-docker
● cri-docker.service - CRI Interface for Docker Application Container Engine
     Loaded: loaded (/etc/systemd/system/cri-docker.service; enabled; vendor preset: disabled)
     Active: active (running) since Tue 2022-12-06 23:31:18 CST; 1min 34s ago
TriggeredBy: ● cri-docker.socket
       Docs: https://docs.mirantis.com
   Main PID: 6861 (cri-dockerd)
      Tasks: 6
     Memory: 50.3M
        CPU: 84ms
     CGroup: /system.slice/cri-docker.service
             └─6861 /usr/local/bin/cri-dockerd --container-runtime-endpoint fd:// --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9

Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Start docker client with request timeout 0s"
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Hairpin mode is set to none"
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Loaded network plugin cni"
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Docker cri networking managed by network plugin cni"
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Docker Info: &{ID:37LI:R2SF:5ZQM:SFBX:6KGE:KFLT:CWY5:ZF4D:LK2C:GSBR:DBJH:K5SA Contain>
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Setting cgroupDriver systemd"
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCid>
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Start cri-dockerd grpc backend"
Dec 06 23:31:18 master systemd[1]: Started CRI Interface for Docker Application Container Engine.
lines 1-22/22 (END)

```

表示安装完成。

# 安装Kubernetes组件

此次安装kubernetes版本为`v1.28.1`

```shell
[root@centos-master ~]# kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.1", GitCommit:"8dc49c4b984b897d423aab4971090e1879eb4f23", GitTreeState:"clean", BuildDate:"2023-08-24T11:21:51Z", GoVersion:"go1.20.7", Compiler:"gc", Platform:"linux/amd64"}

```

repo仓库设置

```powershell
# 1、由于kubernetes的镜像在国外，速度比较慢，这里切换成国内的镜像源
# 2、编辑/etc/yum.repos.d/kubernetes.repo,添加下面的配置

# 阿里源

cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgchech=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
	   http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 华为源

cat <<EOF > /etc/yum.repos.d/kubernetes.repo 
[kubernetes] 
name=Kubernetes 
baseurl=https://repo.huaweicloud.com/kubernetes/yum/repos/kubernetes-el7-$basearch 
enabled=1 
gpgcheck=1 
repo_gpgcheck=0
gpgkey=https://repo.huaweicloud.com/kubernetes/yum/doc/yum-key.gpg https://repo.huaweicloud.com/kubernetes/yum/doc/rpm-package-key.gpg 
EOF

官方源

cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF

# 3、每台机器安装kubeadm、kubelet和kubectl
指定版本
[root@master ~]# yum install --setopt=obsoletes=0 kubeadm-1.17.4-0 kubelet-1.17.4-0 kubectl-1.17.4-0 -y

下载最新版本：
[root@master ~]# yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

# 4、配置kubelet的cgroup
#编辑/etc/sysconfig/kubelet, 添加下面的配置

cat <<EOF | sudo tee /etc/sysconfig/kubelet
KUBELET_CGROUP_ARGS="--cgroup-driver=systemd"
KUBE_PROXY_MODE="ipvs"
EOF

# 5、设置kubelet开机自启
[root@master ~]# systemctl enable kubelet

kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环。
```



# 集群初始化

### 生成kubeadm初始化配置文件

```shell
[root@master ~]# kubeadm config print init-defaults
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4           # 此处修改为master节点的IP地址
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock   # 此处指定使用的container runtime的sock路径
  criSocket: unix:///var/run/cri-dockerd.sock             # 此处指定使用的docker runtime的sock路径
  imagePullPolicy: IfNotPresent
  name: node
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.k8s.io      # 此处为下载镜像的仓库地址
kind: ClusterConfiguration
kubernetesVersion: 1.26.0       # 修改为所需要的K8S版本号
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}

# 或者生成配置文件存储在文件：kubeadm.yaml中
[root@master ~]#  kubeadm config print init-defaults > kubeadm.yaml
```

###### 下载集群所需组件镜像

```shell
# 查看所需要的镜像
[root@master ~]# kubeadm config images list
registry.k8s.io/kube-apiserver:v1.28.1
registry.k8s.io/kube-controller-manager:v1.28.1
registry.k8s.io/kube-scheduler:v1.28.1
registry.k8s.io/kube-proxy:v1.28.1
registry.k8s.io/pause:3.9
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1

```

通过配置文件`kubeadm.yaml`下载kubenetes所需镜像

```shell
[root@centos-master ~]# kubeadm config images pull --config /root/kubeadm.yaml

[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.9
[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.5.9-0
[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.10.1
```

通过配置文件`kubeadm.yaml`初始化集群

```shell
[root@centos-master ~]# kubeadm init --config /root/kubeadm.yaml  --upload-certs

[init] Using Kubernetes version: v1.28.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [centos-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.140]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [centos-master localhost] and IPs [192.168.10.140 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [centos-master localhost] and IPs [192.168.10.140 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 4.526578 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
48f6cf7d44e32a6b4193e0ee36e0301ef95132c4067573cf9b188eeb768d292d
[mark-control-plane] Marking the node centos-master as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node centos-master as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: abcdef.0123456789abcdef
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

#  master节点创建必要文件

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

#  master节点运行

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

#  node节点运行

kubeadm join 192.168.10.140:6443 --token abcdef.0123456789abcdef \
	--discovery-token-ca-cert-hash sha256:be67a796155229a71574a664ec621ac864a3e2abb05e95ca644669692be44f68
```



重置再初始化

```shell
kubeadm reset --cri-socket unix:///var/run/cri-dockerd.sock

rm -fr ~/.kube/  /etc/kubernetes/* var/lib/etcd/*
```

其他master节点加入集群

Token过期后生成新的token：

```shell
kubeadm token create --print-join-command
```



Master需要生成--certificate-key

```shell
[root@k8s-master01 ~]# kubeadm init phase upload-certs  --upload-certs
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
b0af94a5e76fe43000642c322ad376cb057777c2da5cd719a18b48c8458dccbb

```

在其他master节点执行以下命令

```shell
kubeadm join 192.168.211.131:6443 --token abcdef.0123456789abcdef         --discovery-token-ca-cert-hash sha256:4f6bede2b6136cc98db08a6f58727e3d00c39793dd3f97f7d011837be141161a --control-plane --certificate-key b0af94a5e76fe43000642c322ad376cb057777c2da5cd719a18b48c8458dccbb

```





> 下面的操作只需要在node节点上执行即可（node节点加入集群）

```powershell
# 根据提示把worker节点加进master节点，复制你们各自在日志里的提示，然后分别粘贴在2个worker节点上，最后回车即可（注意要在后面加上--cri-socket unix:///var/run/cri-dockerd.sock这一参数，不然可能会失败）

kubeadm join 192.168.211.131:6443 --token abcdef.0123456789abcdef \
        --discovery-token-ca-cert-hash sha256:4f6bede2b6136cc98db08a6f58727e3d00c39793dd3f97f7d011837be141161a --cri-socket unix:///var/run/cri-dockerd.sock


以node01节点为例：
[root@node01 ~]# kubeadm join 192.168.10.130:6443 --token oat3ii.jimyx3sob5cpi15v --discovery-token-ca-cert-hash sha256:fa442bdfc24302c2e355a2e2462a5972db5492f73312c77169d11805941b88df --cri-socket unix:///var/run/cri-dockerd.sock
[preflight] Running pre-flight checks
	[WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```

在master上查看节点信息

```powershell
[root@master ~]# kubectl get nodes
NAME    STATUS   ROLES     AGE   VERSION
master  NotReady  master   6m    v1.17.4
node1   NotReady   <none>  22s   v1.17.4
node2   NotReady   <none>  19s   v1.17.4
```

##### 安装网络插件，只在master节点操作即可

网络插件`flannel`地址：https://github.com/flannel-io

```shell
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
# 或者
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
```



或者使用网络插件`calico`：https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart

```shell
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml
```

等待它安装完毕稍等片刻， 发现集群的状态已经是Ready

### 网络插件排错

#### 故障现象

网络插件的POD无法创建：

```shell
[root@centos-master ~]# kubectl get pod -A -owide
NAMESPACE      NAME                                    READY   STATUS              RESTARTS        AGE   IP               NODE            NOMINATED NODE   READINESS GATES
kube-flannel   kube-flannel-ds-4fdp8                   0/1     CrashLoopBackOff    7 (2m29s ago)   13m   192.168.10.142   centos-node02   <none>           <none>
kube-flannel   kube-flannel-ds-5wtnh                   0/1     CrashLoopBackOff    7 (2m8s ago)    13m   192.168.10.140   centos-master   <none>           <none>
kube-flannel   kube-flannel-ds-bq6hk                   0/1     CrashLoopBackOff    7 (2m2s ago)    13m   192.168.10.141   centos-node01   <none>           <none>
kube-system    coredns-66f779496c-6qfh9                0/1     ContainerCreating   0               45m   <none>           centos-node02   <none>           <none>
kube-system    coredns-66f779496c-wcmn6                0/1     ContainerCreating   0               45m   <none>           centos-node02   <none>           <none>
kube-system    etcd-centos-master                      1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
kube-system    kube-apiserver-centos-master            1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
kube-system    kube-controller-manager-centos-master   1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
kube-system    kube-proxy-26gbn                        1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
kube-system    kube-proxy-kjmfp                        1/1     Running             2 (23m ago)     44m   192.168.10.142   centos-node02   <none>           <none>
kube-system    kube-proxy-n9hr4                        1/1     Running             2 (23m ago)     45m   192.168.10.141   centos-node01   <none>           <none>
kube-system    kube-scheduler-centos-master            1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
```

#### 故障排查

查看POD的描述信息

```shell
[root@centos-master ~]# kubectl describe pod -n kube-flannel kube-flannel-ds-bq6hk
---
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  13m                   default-scheduler  Successfully assigned kube-flannel/kube-flannel-ds-bq6hk to centos-node01
  Normal   Pulled     13m                   kubelet            Container image "docker.io/flannel/flannel-cni-plugin:v1.2.0" already present on machine
  Normal   Created    13m                   kubelet            Created container install-cni-plugin
  Normal   Started    13m                   kubelet            Started container install-cni-plugin
  Normal   Pulled     13m                   kubelet            Container image "docker.io/flannel/flannel:v0.22.2" already present on machine
  Normal   Created    13m                   kubelet            Created container install-cni
  Normal   Started    13m                   kubelet            Started container install-cni
  Normal   Pulled     13m (x4 over 13m)     kubelet            Container image "docker.io/flannel/flannel:v0.22.2" already present on machine
  Normal   Created    13m (x4 over 13m)     kubelet            Created container kube-flannel
  Normal   Started    13m (x4 over 13m)     kubelet            Started container kube-flannel
  Warning  BackOff    3m41s (x47 over 13m)  kubelet            Back-off restarting failed container kube-flannel in pod kube-flannel-ds-bq6hk_kube-flannel(ce487bc6-77d9-47c3-85cb-d8b674ed0a49)

```

查看POD的日志信息

```shell
[root@centos-master ~]# kubectl logs -n kube-flannel kube-flannel-ds-bq6hk
Defaulted container "kube-flannel" out of: kube-flannel, install-cni-plugin (init), install-cni (init)
I0918 07:06:07.259809       1 main.go:212] CLI flags config: {etcdEndpoints:http://127.0.0.1:4001,http://127.0.0.1:2379 etcdPrefix:/coreos.com/network etcdKeyfile: etcdCertfile: etcdCAFile: etcdUsername: etcdPassword: version:false kubeSubnetMgr:true kubeApiUrl: kubeAnnotationPrefix:flannel.alpha.coreos.com kubeConfigFile: iface:[] ifaceRegex:[] ipMasq:true ifaceCanReach: subnetFile:/run/flannel/subnet.env publicIP: publicIPv6: subnetLeaseRenewMargin:60 healthzIP:0.0.0.0 healthzPort:0 iptablesResyncSeconds:5 iptablesForwardRules:true netConfPath:/etc/kube-flannel/net-conf.json setNodeNetworkUnavailable:true useMultiClusterCidr:false}
W0918 07:06:07.259882       1 client_config.go:617] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0918 07:06:07.277157       1 kube.go:145] Waiting 10m0s for node controller to sync
I0918 07:06:07.277218       1 kube.go:489] Starting kube subnet manager
I0918 07:06:08.278091       1 kube.go:152] Node controller sync successful
I0918 07:06:08.278308       1 main.go:232] Created subnet manager: Kubernetes Subnet Manager - centos-node01
I0918 07:06:08.278327       1 main.go:235] Installing signal handlers
I0918 07:06:08.278676       1 main.go:543] Found network config - Backend type: vxlan
I0918 07:06:08.278738       1 match.go:206] Determining IP address of default interface
I0918 07:06:08.280615       1 match.go:259] Using interface with name ens32 and address 192.168.10.141
I0918 07:06:08.280816       1 match.go:281] Defaulting external address to interface address (192.168.10.141)
I0918 07:06:08.280996       1 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
E0918 07:06:08.281602       1 main.go:335] Error registering network: failed to acquire lease: node "centos-node01" pod cidr not assigned   ## 这条是关键，报错pod cidr问题
W0918 07:06:08.282035       1 reflector.go:347] github.com/flannel-io/flannel/pkg/subnet/kube/kube.go:490: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
I0918 07:06:08.282111       1 main.go:523] Stopping shutdownHandler...
```

找到故障信息：`Error registering network: failed to acquire lease: node "centos-node01" pod cidr not assigned`，以此判定是`cidr`问题导致。

#### 解决故障

由于在初始化集群的时候未指定参数：`-pod-network-cidr`造成添加网络插件时报错`Error registering network: failed to acquire lease: node "centos-node01" pod cidr not assigned `，建议重新初始化集群并加上参数：`-pod-network-cidr`

不重新初始化的情况下解决方法如下：

在`/etc/kubernetes/manifests/kube-controller-manager.yaml `中添加`- --allocate-node-cidrs=true`和`- --cluster-cidr=10.244.0.0/16`，然后重启`kubelet`即可。

```shell
[root@centos-master ~]# vim /etc/kubernetes/manifests/kube-controller-manager.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --use-service-account-credentials=true
    - --allocate-node-cidrs=true       #添加
    - --cluster-cidr=10.244.0.0/16     #添加
    image: registry.aliyuncs.com/google_containers/kube-controller-manager:v1.28.2
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-controller-manager
    resources:
      requests:
        cpu: 200m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/pki
      name: etc-pki
      readOnly: true
    - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      name: flexvolume-dir
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /etc/kubernetes/controller-manager.conf
      name: kubeconfig
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/pki
      type: DirectoryOrCreate
    name: etc-pki
  - hostPath:
      path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      type: DirectoryOrCreate
    name: flexvolume-dir
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /etc/kubernetes/controller-manager.conf
      type: FileOrCreate
    name: kubeconfig
status: {}

```

重启`kubelet`

```shell
[root@centos-master ~]# systemctl restart kubelet
```





参考文章



一、参考文档
 【kubernetes】k8s高可用集群搭建（三主三从）_搭建k8s高可用集群-CSDN博客

https://www.cnblogs.com/wangguishe/p/17823687.html#_label13

二、前言
        一直搭建的k8s集群都是1个主节点的学习环境，不适合生产环境，这里介绍一下利用haproxy和keeplived搭建高可用的多主K8s集群。

三、版本计划
角色	ip	软件
kmaster1	192.168.48.210/24	kubeadm1.28.0；kubectl1.28.0；kubelet1.28.0；docker-ce24.0.7，cri-docker0.3.9；ipvsadm v1.31 2019/12/24 (compiled with popt and IPVS v1.2.1)；haproxy:2.3.6；osixia/keepalived:2.0.20
kmaster2	192.168.48.211/24	kubeadm1.28.0；kubectl1.28.0；kubelet1.28.0；docker-ce24.0.7，cri-docker0.3.9；ipvsadm v1.31 2019/12/24 (compiled with popt and IPVS v1.2.1)；haproxy:2.3.6；osixia/keepalived:2.0.20
kmaster3	192.168.48.212/24	kubeadm1.28.0；kubectl1.28.0；kubelet1.28.0；docker-ce24.0.7，cri-docker0.3.9；ipvsadm v1.31 2019/12/24 (compiled with popt and IPVS v1.2.1)；haproxy:2.3.6；osixia/keepalived:2.0.20
knode1	192.168.48.213/24	kubeadm1.28.0；kubectl1.28.0；kubelet1.28.0；docker-ce24.0.7，cri-docker0.3.9；ipvsadm v1.31 2019/12/24 (compiled with popt and IPVS v1.2.1)
knode2	192.168.48.214/24	kubeadm1.28.0；kubectl1.28.0；kubelet1.28.0；docker-ce24.0.7，cri-docker0.3.9；ipvsadm v1.31 2019/12/24 (compiled with popt and IPVS v1.2.1)
knode3	192.168.48.215/24	kubeadm1.28.0；kubectl1.28.0；kubelet1.28.0；docker-ce24.0.7，cri-docker0.3.9；ipvsadm v1.31 2019/12/24 (compiled with popt and IPVS v1.2.1)
knode4	192.168.48.216/24	kubeadm1.28.0；kubectl1.28.0；kubelet1.28.0；docker-ce24.0.7，cri-docker0.3.9；ipvsadm v1.31 2019/12/24 (compiled with popt and IPVS v1.2.1)
        注意：为了避免集群脑裂，建议主节点个数为奇数，具体请参考k8s官网介绍。

四、创建虚拟机
1、所有节点都执行
1）系统初始化
        这里是基于ubuntu22.04克隆的7台设备，需要先系统初始化一下，下面是kmaster1的步骤，其他主机参照步骤执行。

#依次开机，设置ip地址，主机名

## 设置为root登录

sudo su
passwd root

vim /etc/ssh/sshd_config
...
PermitRootLogin yes

sudo service ssh restart

# 修改主机名和hosts

hostnamectl set-hostname kmaster1

vim /etc/hosts
...
192.168.48.210 kmaster1
192.168.48.211 kmaster2
192.168.48.212 kmaster3
192.168.48.213 knode1
192.168.48.214 knode2
192.168.48.215 knode3
192.168.48.216 knode4

## ubuntu22.04环境设置静态地址
ssh ziu@192.168.48.x
sudo su
cp /etc/netplan/00-installer-config.yaml /etc/netplan/00-installer-config.yaml.bbk
vim /etc/netplan/00-installer-config.yaml
network:
  ethernets:
    ens33:
      dhcp4: false
      addresses: [192.168.48.210/24]   ##按照版本计划表修改ip
      optional: true
      routes:
        - to: default
          via: 192.168.48.2
      nameservers: 
        addresses: [192.168.48.2]
  version: 2

netplan apply
2）设置转发IPV4并让iptables看到桥接流量
# br_netfilter是一个内核模块，它允许在网桥设备上加入防火墙功能，因此也被称为透明防火墙或桥接模式防火墙。这种防火墙具有部署能力强，隐蔽性好，安全性高的有点
# overlay模块则是用于支持overlay网络文件系统的模块，overlay文件系统是一种在现有文件系统的顶部创建叠加层的办法，以实现联合挂载(Union Mount)。它允许将多个文件系统合并为一个单一的逻辑文件系统，具有层次结构和优先级，这样可以方便的将多个文件系统中的文件或目录合并在一起，而不需要实际复制和移动文件
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# 设置所需的 sysctl 参数，参数在重新启动后保持不变
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# 应用 sysctl 参数而不重新启动
sudo sysctl --system


#通过运行以下指令确认 br_netfilter 和 overlay 模块被加载：

lsmod | grep br_netfilter
lsmod | grep overlay

#通过运行以下指令确认 net.bridge.bridge-nf-call-iptables、net.bridge.bridge-nf-call-ip6tables 和 net.ipv4.ip_forward 系统变量在你的 sysctl 配置中被设置为 1：

sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward
3）设置ipvs
（以下使用ipvs的原因是问的AI的解答）

Kubernetes（k8s）多主集群通常建议使用IPVS（IP Virtual Server）作为服务代理，原因主要包括以下几点：

（1）性能优势：
   IPVS内置于Linux内核中，其基于哈希表的快速查找机制在大规模高并发场景下比iptables（Netfilter）的NAT模式具有更高的性能和更低的延迟。

（2）负载均衡算法丰富：
   IPVS支持丰富的负载均衡算法，包括轮询、最少连接、源哈希、加权轮询等，可以根据业务需求灵活选择合适的负载均衡策略。

（3）会话保持能力：
   IPVS支持多种会话保持方式，例如源IP地址会话保持、目的IP地址会话保持、TCP/UDP端口会话保持等，这对于保持客户端与后端服务之间的长连接或状态相关的应用至关重要。

（4）更好的可扩展性和稳定性：
   在大规模集群中，随着Service数量的增长，iptables规则的数量也会迅速增加，这可能会影响系统的稳定性和性能。而IPVS则通过内核态实现高效的负载均衡和转发，避免了这个问题。

（5）更精细的服务管理：
   IPVS可以对单个服务进行粒度更细的管理，如单独设置服务的健康检查、权重调整等，更适合复杂的云原生环境。

（6）支持集群内部通信：
   在k8s集群内部，kube-proxy组件利用IPVS可以更好地处理Pod间的通信和服务发现。

因此，在构建Kubernetes多主集群时，使用IPVS能够提供更高效率、更稳定的网络服务代理功能，以满足分布式系统中的高可用和高性能需求。不过，IPVS并不是必须选项，kube-proxy也支持iptables和userspace两种模式，但在大规模生产环境中，IPVS往往是首选方案。

注意：前面我参考的文档也有ipvs步骤，参数比下面步骤少，具体看自己设置吧。

#安装ipvs

sudo apt update
sudo apt install ipvsadm ipset sysstat conntrack libseccomp2 -y


cat > /etc/modules-load.d/ipvs.conf << EOF
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
EOF

 

sudo modprobe ip_vs
sudo modprobe ip_vs_rr
sudo modprobe ip_vs_wrr
sudo modprobe ip_vs_sh
sudo modprobe nf_conntrack
sudo modprobe ip_tables
sudo modprobe ip_set
sudo modprobe xt_set
sudo modprobe ipt_set
sudo modprobe ipt_rpfilter
sudo modprobe ipt_REJECT
sudo modprobe ipip

#通过运行以下指令确认模块被加载：

lsmod | grep ip_vs
lsmod | grep ip_vs_rr
lsmod | grep ip_vs_wrr
lsmod | grep ip_vs_sh
lsmod | grep nf_conntrack
lsmod | grep ip_tables
lsmod | grep ip_set
lsmod | grep xt_set
lsmod | grep ipt_set
lsmod | grep ipt_rpfilter
lsmod | grep ipt_REJECT
lsmod | grep ipip
4）安装docker-ce
# 官网提供的步骤
# https://docs.docker.com/engine/install/ubuntu/

for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done


# Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl gnupg -y
#sudo install -m 0755 -d /etc/apt/keyrings


curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

# Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update


sudo apt install docker-ce docker-ce-cli containerd.io 
#sudo apt install docker-buildx-plugin docker-compose-plugin  #可以不按照，不是k8s集群必须

#防止在执行apt update和apt upgrade时自动更新
sudo apt-mark hold docker-ce docker-ce-cli containerd.io 


docker version
Client: Docker Engine - Community
 Version:           24.0.7
 API version:       1.43
 Go version:        go1.20.10
 Git commit:        afdd53b
 Built:             Thu Oct 26 09:07:41 2023
 OS/Arch:           linux/amd64
 Context:           default

Server: Docker Engine - Community
 Engine:
  Version:          24.0.7
  API version:      1.43 (minimum version 1.12)
  Go version:       go1.20.10
  Git commit:       311b9ff
  Built:            Thu Oct 26 09:07:41 2023
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.6.26
  GitCommit:        3dd1e886e55dd695541fdcd67420c2888645a495
 runc:
  Version:          1.1.10
  GitCommit:        v1.1.10-0-g18a0cb0
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
5）安装容器进行时cri-docker
# https://github.com/Mirantis/cri-dockerd/releases
# Run these commands as root
## 如果github下载失败，gitee上有同步过来的0.3.8的，也可直接用

wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.9/cri-dockerd-0.3.9.amd64.tgz
tar -xf cri-dockerd-0.3.9.amd64.tgz
cd cri-dockerd
mkdir -p /usr/local/bin
install -o root -g root -m 0755 cri-dockerd /usr/local/bin/cri-dockerd
#install packaging/systemd/* /etc/systemd/system
#sed -i -e 's,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,' /etc/systemd/system/cri-docker.service

##编写service-------------------------------------------------------------------------------------
vim /etc/systemd/system/cri-docker.service
[Unit]
Description=CRI Interface for Docker Application Container Engine
Documentation=https://docs.mirantis.com
After=network-online.target firewalld.service docker.service
Wants=network-online.target
Requires=cri-docker.socket

[Service]
Type=notify
ExecStart=/usr/local/bin/cri-dockerd --container-runtime-endpoint fd:// --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9
ExecReload=/bin/kill -s HUP $MAINPID
TimeoutSec=0
RestartSec=2
Restart=always

# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
# Both the old, and new location are accepted by systemd 229 and up, so using the old location
# to make them work for either version of systemd.
StartLimitBurst=3

# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
# this option work for either version of systemd.
StartLimitInterval=60s

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Comment TasksMax if your systemd version does not support it.
# Only systemd 226 and above support this option.
TasksMax=infinity
Delegate=yes
KillMode=process

[Install]
WantedBy=multi-user.target

##编写socket---------------------------------------------------------------------------------
vim /etc/systemd/system/cri-docker.socket
[Unit]
Description=CRI Docker Socket for the API
PartOf=cri-docker.service

[Socket]
ListenStream=%t/cri-dockerd.sock
SocketMode=0660
SocketUser=root
SocketGroup=docker

[Install]
WantedBy=sockets.target


##加载service配置并设置开机自启
systemctl daemon-reload
systemctl enable --now cri-docker.socket


#插叙cgroup配置，默认就是systemd，所以不用修改了
docker info | grep -i cgroup
 Cgroup Driver: systemd
 Cgroup Version: 2
  cgroupns
6）安装kubeadm，kubelet，kubectl
（1）安装前检查
一台兼容的linux主机

单主机节点内存2GB以上，CPU2核以上

集群主机节点之间网络互联

检查容器进行时

#下面截图的容器进行时任选一个
root@kmaster1:~/cri-dockerd# ls /var/run/cri-dockerd.sock 
/var/run/cri-dockerd.sock
节点之间不可以有重复主机名，mac地址，product_uuid

#检查网口和MAC地址
ip link

#检查product_uuid
sudo cat /sys/class/dmi/id/product_uuid

#检查主机名
hostname
开启端口：6443

#主机端口，这里检查没有输出
nc 127.0.0.1 6443
禁用交换分区

#注释里面swap的行
vim /etc/fstab 

#临时关闭
swapoff -a

#查看时间是否一致
date

#关闭防火墙
# 关闭ufw和firewalled
systemctl stop ufw firewalld
systemctl disable ufw firewalld

# 禁用selinux（ubuntu没有启用，centos才默认启用，需要注意一下）
#默认ubunt默认是不安装selinux的，如果没有selinux命令和配置文件则说明没有安装selinux，则下面步骤就不用做了
sed -ri 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config 
setenforce 0
（2）安装3个命令
#此次的安装步骤
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -

cat >/etc/apt/sources.list.d/kubernetes.list <<EOF 
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF

mv /etc/apt/sources.list.d/docker.list /root/
apt update

sudo apt list kubeadm -a
sudo apt-get install -y kubelet=1.28.0-00 kubeadm=1.28.0-00 kubectl=1.28.0-00
sudo apt-mark hold kubelet kubeadm kubectl


#---------以下安装源可任选一种----------------------------------------
#阿里云的源
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -

cat >/etc/apt/sources.list.d/kubernetes.list <<EOF 
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF

#华为云的源
curl -fsSL  https://repo.huaweicloud.com/kubernetes/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg


echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://repo.huaweicloud.com/kubernetes/apt/ kubernetes-xenial main' | sudo tee /etc/apt/sources.list.d/kubernetes.list


#k8s官方源
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg


echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
2、3个主节点执行
1）设置命令补全
apt install bash-completion -y
echo "source <(kubectl completion bash)" >> ~/.bashrc

source .bashrc
2）安装haproxy-基于docker方式
#创建haproxy配置文件

mkdir /etc/haproxy
vim /etc/haproxy/haproxy.cfg
root@kmaster1:~# grep -v '^#' /etc/haproxy/haproxy.cfg

global
    # to have these messages end up in /var/log/haproxy.log you will
    # need to:
    #
    # 1) configure syslog to accept network log events.  This is done
    #    by adding the '-r' option to the SYSLOGD_OPTIONS in
    #    /etc/sysconfig/syslog
    #
    # 2) configure local2 events to go to the /var/log/haproxy.log
    #   file. A line like the following can be added to
    #   /etc/sysconfig/syslog
    #
    #    local2.*                       /var/log/haproxy.log
    #
    log         127.0.0.1 local2

    pidfile     /var/run/haproxy.pid
    maxconn     4000
    # daemon
     
    # turn on stats unix socket
    stats socket /var/lib/haproxy/stats

defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

frontend  kubernetes-apiserver
    mode tcp
    bind *:9443  ## 监听9443端口
    # bind *:443 ssl # To be completed ....

    acl url_static       path_beg       -i /static /images /javascript /stylesheets
    acl url_static       path_end       -i .jpg .gif .png .css .js
     
    default_backend             kubernetes-apiserver

backend kubernetes-apiserver
    mode        tcp  # 模式tcp
    balance     roundrobin  # 采用轮询的负载算法
 server kmaster1 192.168.48.210:6443 check
 server kmaster2 192.168.48.211:6443 check
 server kmaster3 192.168.48.212:6443 check

 

root@kmaster1:~# docker run -d \
--restart always \
--name=haproxy \
--net=host  \
-v /etc/haproxy:/usr/local/etc/haproxy:ro \
-v /var/lib/haproxy:/var/lib/haproxy \
haproxy:2.3.6

root@kmaster1:~# docker ps -a
3）安装keepalived-基于docker方式
#查看网口名，这里是ens33，自己根据自己的网口名称修改
root@kmaster1:~# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000
    link/ether 00:0c:29:e7:89:b3 brd ff:ff:ff:ff:ff:ff
    altname enp2s1
3: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1480 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default 
    link/ether 02:42:58:31:78:56 brd ff:ff:ff:ff:ff:ff
5: kube-ipvs0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN mode DEFAULT group default 
    link/ether 8e:f2:89:c8:e1:d5 brd ff:ff:ff:ff:ff:ff


#创建keepalived配置文件

mkdir /etc/keepalived
vim /etc/keepalived/keepalived.conf
global_defs {
   script_user root 
   enable_script_security

}

vrrp_script chk_haproxy {
    script "/bin/bash -c 'if [[ $(netstat -nlp | grep 9443) ]]; then exit 0; else exit 1; fi'"  # haproxy 检测脚本，这里需要根据自己实际情况判断
    interval 2  # 每2秒执行一次检测
    weight 11 # 权重变化
}

vrrp_instance VI_1 {
  interface ens33 # 此处通过ip addr命令根据实际填写

  state MASTER # backup节点设为BACKUP
  virtual_router_id 51 # id设为相同，表示是同一个虚拟路由组
  priority 100 #初始权重

  #这里需要注意，我的3个Master节点不在同一个网段，不配置会出现多个Master节点的脑裂现象，值根据当前节点情况，配置其余2个节点
 # unicast_peer {
#		192.168.48.210
#		192.168.48.211
#		192.168.48.212
 # }

  virtual_ipaddress {
    192.168.48.222  # vip 虚拟ip
  }

  authentication {
    auth_type PASS
    auth_pass password
  }

  track_script {
      chk_haproxy
  }

  notify "/container/service/keepalived/assets/notify.sh"
}
docker run --cap-add=NET_ADMIN \
--restart always \
--name keepalived \
--cap-add=NET_BROADCAST \
--cap-add=NET_RAW \
--net=host \
--volume /etc/keepalived/keepalived.conf:/container/service/keepalived/assets/keepalived.conf \
-d osixia/keepalived:2.0.20 \
--copy-service

docker ps -a
4）验证vip的切换
前提：3个主节点都配置完成了haproxy和keepalived，并容器启动成功

#vip所在的master节点

root@kmaster1:~# ip a s
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 00:0c:29:e7:89:b3 brd ff:ff:ff:ff:ff:ff
    altname enp2s1
    inet 192.168.48.212/24 brd 192.168.48.255 scope global ens33
       valid_lft forever preferred_lft forever
    inet 192.168.48.222/32 scope global ens33
       valid_lft forever preferred_lft forever
    inet6 fe80::20c:29ff:fee7:89b3/64 scope link 
       valid_lft forever preferred_lft forever
3: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
    inet 10.244.163.64/32 scope global tunl0
       valid_lft forever preferred_lft forever
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:7a:40:ff:74 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: kube-ipvs0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default 
    link/ether 8e:f2:89:c8:e1:d5 brd ff:ff:ff:ff:ff:ff
    inet 10.96.0.10/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.0.1/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.97.78.110/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever


#非vip的master节点
root@kmaster2:~# ip a s
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 00:0c:29:a6:69:6e brd ff:ff:ff:ff:ff:ff
    altname enp2s1
    inet 192.168.48.211/24 brd 192.168.48.255 scope global ens33
       valid_lft forever preferred_lft forever
    inet6 fe80::20c:29ff:fea6:696e/64 scope link 
       valid_lft forever preferred_lft forever
3: tunl0@NONE: <NOARP,UP,LOWER_UP> mtu 1480 qdisc noqueue state UNKNOWN group default qlen 1000
    link/ipip 0.0.0.0 brd 0.0.0.0
    inet 10.244.135.2/32 scope global tunl0
       valid_lft forever preferred_lft forever
4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 02:42:4c:cd:64:63 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
5: kube-ipvs0: <BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN group default 
    link/ether 8e:f2:89:c8:e1:d5 brd ff:ff:ff:ff:ff:ff
    inet 10.96.0.1/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.96.0.10/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
    inet 10.97.78.110/32 scope global kube-ipvs0
       valid_lft forever preferred_lft forever
10: calibc082d1a6a4@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default qlen 1000
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link 
       valid_lft forever preferred_lft forever
11: cali41e20c84d22@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1480 qdisc noqueue state UP group default qlen 1000
    link/ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netnsid 1
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link 
       valid_lft forever preferred_lft forever
5）拉取集群需要的镜像
        这里以kmaster1举例，kmaster2和kmaster3参照执行。

#查看镜像版本
root@kmaster1:~# kubeadm config images list
I0117 08:26:18.644762    4685 version.go:256] remote version is much newer: v1.29.0; falling back to: stable-1.28
registry.k8s.io/kube-apiserver:v1.28.5
registry.k8s.io/kube-controller-manager:v1.28.5
registry.k8s.io/kube-scheduler:v1.28.5
registry.k8s.io/kube-proxy:v1.28.5
registry.k8s.io/pause:3.9
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1

#查看阿里云的镜像库的镜像版本
root@kmaster1:~# kubeadm config images list --image-repository registry.aliyuncs.com/google_containers
I0117 08:28:40.181207    4779 version.go:256] remote version is much newer: v1.29.0; falling back to: stable-1.28
registry.aliyuncs.com/google_containers/kube-apiserver:v1.28.5
registry.aliyuncs.com/google_containers/kube-controller-manager:v1.28.5
registry.aliyuncs.com/google_containers/kube-scheduler:v1.28.5
registry.aliyuncs.com/google_containers/kube-proxy:v1.28.5
registry.aliyuncs.com/google_containers/pause:3.9
registry.aliyuncs.com/google_containers/etcd:3.5.9-0
registry.aliyuncs.com/google_containers/coredns:v1.10.1

#下载镜像
root@kmaster1:~# kubeadm config images pull --kubernetes-version=v1.28.0 --image-repository registry.aliyuncs.com/google_containers  --cri-socket unix:///var/run/cri-dockerd.sock
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.28.0
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.28.0
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.28.0
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.28.0
[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.9
[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.5.9-0
[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.10.1
3、kmaster1节点执行
        创建集群初始化的配置文件。

#kmaster1上创建k8s-init-config.yaml

vim k8s-init-config.yaml

-----------------------整个文件内容--------------------------------
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: wgs001.com3yjucgqr276rf # 可以自定义，正则([a-z0-9]{6}).([a-z0-9]{16})
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
  kind: InitConfiguration
  localAPIEndpoint:
  advertiseAddress: 192.168.48.210 # 修改成节点ip
  bindPort: 6443
  nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  imagePullPolicy: IfNotPresent
  name: kmaster1 # 节点的hostname
  taints: 
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
---
apiServer:
  timeoutForControlPlane: 4m0s
  certSANs: # 3主个节点IP和vip的ip
  - 192.168.48.210
  - 192.168.48.211
  - 192.168.48.212
  - 192.168.48.222
apiVersion: kubeadm.k8s.io/v1beta3
controlPlaneEndpoint: "192.168.48.222:6443" # 设置vip高可用地址
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
    local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers # 设置国内源
kind: ClusterConfiguration
kubernetesVersion: v1.28.0 # 指定版本
networking:
    dnsDomain: k8s.local
    podSubnet: 10.244.0.0/16 # 增加指定pod的网段
    serviceSubnet: 10.96.0.0/12
scheduler: {}
---
# 用于配置kube-proxy上为Service指定的代理模式: ipvs or iptables
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "ipvs"
---
# 指定cgroup
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: "systemd"
执行结果

#kmaster1
root@kmaster1:~# kubeadm init --config kubeadm-init-config.yaml  --upload-certs
[init] Using Kubernetes version: v1.28.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kmaster1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.k8s.local] and IPs [10.96.0.1 192.168.48.210 192.168.48.222 192.168.48.211 192.168.48.212]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [kmaster1 localhost] and IPs [192.168.48.210 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [kmaster1 localhost] and IPs [192.168.48.210 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 10.505761 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
b5649c4e771848c33ffeaa3e18c2dc59e94da94c0a3b98c7463421bf2f1810b5
[mark-control-plane] Marking the node kmaster1 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kmaster1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: wgs001.com3yjucgqr276rf
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 192.168.48.222:6443 --token wgs001.com3yjucgqr276rf \
	--discovery-token-ca-cert-hash sha256:a64483862c6408f4cde941f375d27ea9fa9ee5012ff37be2acd8c8436c09148d \
	--control-plane --certificate-key b5649c4e771848c33ffeaa3e18c2dc59e94da94c0a3b98c7463421bf2f1810b5

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.48.222:6443 --token wgs001.com3yjucgqr276rf \
	--discovery-token-ca-cert-hash sha256:a64483862c6408f4cde941f375d27ea9fa9ee5012ff37be2acd8c8436c09148d 


root@kmaster1:~# mkdir -p $HOME/.kube
root@kmaster1:~# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
root@kmaster1:~# sudo chown $(id -u):$(id -g) $HOME/.kube/config
4、kmaster2和kmaster3执行
kubeadm join 192.168.48.222:6443 --token wgs001.com3yjucgqr276rf \
	--discovery-token-ca-cert-hash sha256:a64483862c6408f4cde941f375d27ea9fa9ee5012ff37be2acd8c8436c09148d \
	--control-plane --certificate-key b5649c4e771848c33ffeaa3e18c2dc59e94da94c0a3b98c7463421bf2f1810b5  --cri-socket unix:///var/run/cri-dockerd.sock

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
5、knode1到knode4执行
kubeadm join 192.168.48.222:6443 --token wgs001.com3yjucgqr276rf \
	--discovery-token-ca-cert-hash sha256:a64483862c6408f4cde941f375d27ea9fa9ee5012ff37be2acd8c8436c09148d  --cri-socket unix:///var/run/cri-dockerd.sock
6、安装calico
请参考我的另一个文档的ubuntu22.04安装k8s学习环境1主2从(cri-docker方式)-CSDN博客calico步骤，calico.yaml的内容太多，就不重复叙述了。

下载github的calico文件的，请到以下地址获取3.27版本

https://github.com/projectcalico/calico.git

git下来的文件路径：calico/manifests/calico.yaml

root@kmaster1:~# kubectl apply -f calico.yaml
poddisruptionbudget.policy/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
serviceaccount/calico-node created
serviceaccount/calico-cni-plugin created
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrole.rbac.authorization.k8s.io/calico-cni-plugin created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-cni-plugin created
daemonset.apps/calico-node created
deployment.apps/calico-kube-controllers created

##查看集群节点状态
root@kmaster1:~# kubectl get nodes 
NAME       STATUS   ROLES           AGE     VERSION
kmaster1   Ready    control-plane   6h12m   v1.28.0
kmaster2   Ready    control-plane   6h9m    v1.28.0
kmaster3   Ready    control-plane   6h8m    v1.28.0
knode1     Ready    <none>          6h8m    v1.28.0
knode2     Ready    <none>          6h8m    v1.28.0
knode3     Ready    <none>          6h8m    v1.28.0
knode4     Ready    <none>          6h8m    v1.28.0
7、设置主节点的etcd高可用
#检查所有master节点，一般kmaster1改了之后，其他节点会同步，只检查即可，修改后，重启一下kubelet
vim /etc/kubernetes/manifests/etcd.yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.48.212:2379
  creationTimestamp: null
  labels:
    component: etcd
    tier: control-plane
  name: etcd
  namespace: kube-system
spec:
  containers:
  - command:
    - etcd
    - --advertise-client-urls=https://192.168.48.212:2379
    - --cert-file=/etc/kubernetes/pki/etcd/server.crt
    - --client-cert-auth=true
    - --data-dir=/var/lib/etcd
    - --experimental-initial-corrupt-check=true
    - --experimental-watch-progress-notify-interval=5s
    - --initial-advertise-peer-urls=https://192.168.48.212:2380
    - --initial-cluster=kmaster2=https://192.168.48.211:2380,kmaster1=https://192.168.48.210:2380,kmaster3=https://192.168.48.212:2380  ##看这里
    ...

    
    

#重启kubelet
systemctl restart kubelet
五、集群高可用测试
#创建测试pod，创建成功后会进入容器

kubectl run client --image=ikubernetes/admin-box -it --rm --restart=Never --command -n default -- /bin/bash


#ping测试
ping www.baidu.com


#nslookup测试域名
nslookup  www.baidu.com


#将3个主节点依次关机测试vip192.168.48.222的变化
ip a s
六、其他操作（生产环境慎用）
注意：集群创建有问题，或者加入集群有问题，可以参考一下步骤重置集群。

#驱逐节点的配置
kubectl drain knode3 --delete-emptydir-data --force --ignore-daemonsets 


#重置当前节点配置
kubeadm reset -f --cri-socket unix:var/run/cri-dockerd.sock


#删除节点
kubectl delete node knode3

#删除配置信息
rm -rf /etc/cni/net.d  /root/.kube
ipvsadm --clear
