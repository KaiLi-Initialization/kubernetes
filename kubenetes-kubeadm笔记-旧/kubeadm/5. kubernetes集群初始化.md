### 方式一：yaml文件

#### 文件生成

生成kubernetes集群初始化文件**`kubeadm.yaml`**

```shell
kubeadm config print init-defaults > kubeadm.yaml
```



**`kubeadm.yaml`**文件详解

```shell
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: wgs001.com3yjucgqr276rf # 可以自定义，正则([a-z0-9]{6}).([a-z0-9]{16})
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.10.131 # 修改成master节点ip
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  imagePullPolicy: IfNotPresent
  name: k8s-master01 # 节点的hostname
  taints: 
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
---
apiServer:
  timeoutForControlPlane: 4m0s
  certSANs: # 3主个节点IP和vip的ip
  - 192.168.48.210
  - 192.168.48.211
  - 192.168.48.212
  - 192.168.48.222
apiVersion: kubeadm.k8s.io/v1beta3
controlPlaneEndpoint: "192.168.48.222:6443" # 设置vip高可用地址
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers # 设置国内源
kind: ClusterConfiguration
kubernetesVersion: v1.28.0 # 指定版本
networking:
  dnsDomain: k8s.local
  podSubnet: 10.244.0.0/16 # 增加指定pod的网段
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
# 用于配置kube-proxy上为Service指定的代理模式: ipvs or iptables
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "ipvs"
---
# 指定cgroup
# 从 v1.22 开始，在使用 kubeadm 创建集群时，如果用户没有在 KubeletConfiguration 下设置 cgroupDriver 字段，kubeadm 默认使用 systemd。

apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: "systemd"
```



#### 镜像下载

下载集群所需组件镜像

```shell
# 查看所需要的镜像
[root@master ~]# kubeadm config images list
registry.k8s.io/kube-apiserver:v1.28.1
registry.k8s.io/kube-controller-manager:v1.28.1
registry.k8s.io/kube-scheduler:v1.28.1
registry.k8s.io/kube-proxy:v1.28.1
registry.k8s.io/pause:3.9
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1

```



通过配置文件`kubeadm.yaml`下载kubenetes所需镜像

```shell
[root@centos-master ~]# kubeadm config images pull --config /root/kubeadm.yaml

[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.9
[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.5.9-0
[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.10.1
```



#### 集群初始化

通过配置文件`kubeadm.yaml`初始化集群

```shell
[root@centos-master ~]# kubeadm init --config /root/kubeadm.yaml  --upload-certs

[init] Using Kubernetes version: v1.28.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [centos-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.140]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [centos-master localhost] and IPs [192.168.10.140 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [centos-master localhost] and IPs [192.168.10.140 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 4.526578 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
48f6cf7d44e32a6b4193e0ee36e0301ef95132c4067573cf9b188eeb768d292d
[mark-control-plane] Marking the node centos-master as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node centos-master as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: abcdef.0123456789abcdef
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

#  master节点创建必要文件

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

#  master节点运行

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

#  node节点运行

kubeadm join 192.168.10.140:6443 --token abcdef.0123456789abcdef \
	--discovery-token-ca-cert-hash sha256:be67a796155229a71574a664ec621ac864a3e2abb05e95ca644669692be44f68
```



重置再初始化

```shell
kubeadm reset --cri-socket unix:///var/run/cri-dockerd.sock

rm -fr ~/.kube/  /etc/kubernetes/* var/lib/etcd/*
```



#### master节点加入集群

在现有的 master 节点上运行以下命令 `kubeadm join` 来获取及其必要的 token 和 CA 证书哈希：

1. Token过期后生成新的token：

   ```shell
   kubeadm token create --print-join-command
   ```

   该命令将输出一个 `kubeadm join` 命令，例如：

   ```shell
   kubeadm join <master-ip>:<port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>
   ```

   

2. Master需要生成证书密钥--certificate-key

   **这个密钥用于在新的 master 节点上加入集群，证书密钥（Certificate Key）用于在集群的控制平面节点之间安全地共享证书和密钥。这在设置高可用性（HA）控制平面时尤其重要。**

   ```shell
   [root@k8s-master01 ~]# kubeadm init phase upload-certs  --upload-certs
   [upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
   [upload-certs] Using certificate key:
   b0af94a5e76fe43000642c322ad376cb057777c2da5cd719a18b48c8458dccbb
   ```

   

<<<<<<< HEAD
错误提示：

没有参数：controlPlaneEndpoint address

```shell

[root@k8s-master02 ~]# kubeadm join 192.168.211.131:6443 --token abcdef.0123456789abcdef >         --discovery-token-ca-cert-hash sha256:27108633cde5fb07c1d8d0a3d5972043e0f16504c3947ef2c9c2ff5d2a7a78af --control-plane --certificate-key f1b866bc6e115cbe24141c49a9fa4ee1c375bac7c0cdf3bfe70ece849f305621
accepts at most 1 arg(s), received 2
To see the stack trace of this error execute with --v=5 or higher
[root@k8s-master02 ~]# kubeadm join 192.168.211.131:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:27108633cde5fb07c1d8d0a3d5972043e0f16504c3947ef2c9c2ff5d2a7a78af --control-plane --certificate-key f1b866bc6e115cbe24141c49a9fa4ee1c375bac7c0cdf3bfe70ece849f305621
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
error execution phase preflight:
One or more conditions for hosting a new control plane instance is not satisfied.

unable to add a new control plane instance to a cluster that doesn't have a stable controlPlaneEndpoint address

Please ensure that:
* The cluster has a stable controlPlaneEndpoint address.
* The certificates that must be shared among control plane instances are provided.


To see the stack trace of this error execute with --v=5 or higher

```

参考文档：https://blog.csdn.net/hedao0515/article/details/126342939

3. 将其他master节点加入集群

   ```shell
   kubeadm join 192.168.211.131:6443 --token abcdef.0123456789abcdef         --discovery-token-ca-cert-hash sha256:4f6bede2b6136cc98db08a6f58727e3d00c39793dd3f97f7d011837be141161a --control-plane --certificate-key b0af94a5e76fe43000642c322ad376cb057777c2da5cd719a18b48c8458dccbb
   ```
>>>>>>> ada0b919b3fa58d7b4edc605fa177ba8702697a4

   

#### node节点加入集群

根据提示把worker节点加进master节点，复制你们各自在日志里的提示，然后分别粘贴在2个worker节点上，最后回车即可（注意要在后面加上--cri-socket unix:///var/run/cri-dockerd.sock这一参数，不然可能会失败）

```shell
kubeadm join 192.168.211.131:6443 --token abcdef.0123456789abcdef \
        --discovery-token-ca-cert-hash sha256:4f6bede2b6136cc98db08a6f58727e3d00c39793dd3f97f7d011837be141161a --cri-socket unix:///var/run/cri-dockerd.sock
        
[preflight] Running pre-flight checks
	[WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```



### 方式二：命令方式

####  下载镜像

```powershell
# 在安装kubernetes集群之前，必须要提前准备好集群需要的镜像，所需镜像可以通过下面命令查看
[root@master ~]# kubeadm config images list

kubeadm config images list --kubernetes-version v1.25.4

registry.k8s.io/kube-apiserver:v1.26.0
registry.k8s.io/kube-controller-manager:v1.26.0
registry.k8s.io/kube-scheduler:v1.26.0
registry.k8s.io/kube-proxy:v1.26.0
registry.k8s.io/pause:3.9
registry.k8s.io/etcd:3.5.6-0
registry.k8s.io/coredns/coredns:v1.9.3

#查看国内镜像
kubeadm config images list --kubernetes-version v1.25.4 --image-repository registry.aliyuncs.com/google_containers

registry.aliyuncs.com/google_containers/kube-apiserver:v1.26.0
registry.aliyuncs.com/google_containers/kube-controller-manager:v1.26.0
registry.aliyuncs.com/google_containers/kube-scheduler:v1.26.0
registry.aliyuncs.com/google_containers/kube-proxy:v1.26.0
registry.aliyuncs.com/google_containers/pause:3.9
registry.aliyuncs.com/google_containers/etcd:3.5.6-0
registry.aliyuncs.com/google_containers/coredns:v1.9.3

推荐使用以下方式（此处是kubenetes v1.25.4版本为例）
docker pull registry.aliyuncs.com/google_containers/kube-apiserver:v1.26.0
docker tag registry.aliyuncs.com/google_containers/kube-apiserver:v1.26.0  registry.k8s.io/kube-apiserver:v1.26.0
docker rmi registry.aliyuncs.com/google_containers/kube-apiserver:v1.26.0

docker pull registry.aliyuncs.com/google_containers/kube-controller-manager:v1.26.0
docker tag registry.aliyuncs.com/google_containers/kube-controller-manager:v1.26.0  registry.k8s.io/kube-controller-manager:v1.26.0
docker rmi registry.aliyuncs.com/google_containers/kube-controller-manager:v1.26.0

docker pull registry.aliyuncs.com/google_containers/kube-scheduler:v1.26.0
docker tag registry.aliyuncs.com/google_containers/kube-scheduler:v1.26.0 registry.k8s.io/kube-scheduler:v1.26.0
docker rmi registry.aliyuncs.com/google_containers/kube-scheduler:v1.26.0

docker pull registry.aliyuncs.com/google_containers/kube-proxy:v1.26.0
docker tag registry.aliyuncs.com/google_containers/kube-proxy:v1.26.0 registry.k8s.io/kube-proxy:v1.26.0
docker rmi registry.aliyuncs.com/google_containers/kube-proxy:v1.26.0

docker pull registry.aliyuncs.com/google_containers/pause:3.9
docker tag registry.aliyuncs.com/google_containers/pause:3.9 registry.k8s.io/pause:3.9
docker rmi registry.aliyuncs.com/google_containers/pause:3.9

docker pull registry.aliyuncs.com/google_containers/etcd:3.5.6-0
docker tag registry.aliyuncs.com/google_containers/etcd:3.5.6-0 registry.k8s.io/etcd:3.5.6-0
docker rmi registry.aliyuncs.com/google_containers/etcd:3.5.6-0

docker pull registry.aliyuncs.com/google_containers/coredns:v1.9.3
docker tag registry.aliyuncs.com/google_containers/coredns:v1.9.3 registry.k8s.io/coredns/coredns:v1.9.3
docker rmi registry.aliyuncs.com/google_containers/coredns:v1.9.3

```

#### 集群初始化

>下面的操作只需要在master节点上执行即可

```powershell
# 创建集群
[root@master ~]# kubeadm init \
	--apiserver-advertise-address=192.168.10.181 \
	--image-repository registry.aliyuncs.com/google_containers \
	--kubernetes-version=v1.28.0 \
	--service-cidr=10.96.0.0/12 \
	--pod-network-cidr=10.244.0.0/16 \
	--cri-socket unix://var/run/cri-dockerd.sock \
	--ignore-preflight-errors=Numcpu

```

显示以下内容为安装完成

```shell
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

#  master节点创建必要文件

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

#  master节点运行

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

#  node节点运行

kubeadm join 192.168.10.140:6443 --token abcdef.0123456789abcdef \
	--discovery-token-ca-cert-hash sha256:be67a796155229a71574a664ec621ac864a3e2abb05e95ca644669692be44f68

```



重置再初始化

```shell
kubeadm reset --cri-socket unix:///var/run/cri-dockerd.sock

rm -fr ~/.kube/  /etc/kubernetes/* var/lib/etcd/*
```



master节点加入集群

Token过期后生成新的token：

```shell
kubeadm token create --print-join-command
```



Master需要生成--certificate-key

```shell
[root@k8s-master01 ~]# kubeadm init phase upload-certs  --upload-certs
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
b0af94a5e76fe43000642c322ad376cb057777c2da5cd719a18b48c8458dccbb

```

在其他master节点执行以下命令

```shell
kubeadm join 192.168.211.131:6443 --token abcdef.0123456789abcdef         --discovery-token-ca-cert-hash sha256:4f6bede2b6136cc98db08a6f58727e3d00c39793dd3f97f7d011837be141161a --control-plane --certificate-key b0af94a5e76fe43000642c322ad376cb057777c2da5cd719a18b48c8458dccbb

```



node节点加入集群

根据提示把worker节点加进master节点，复制你们各自在日志里的提示，然后分别粘贴在2个worker节点上，最后回车即可（注意要在后面加上--cri-socket unix:///var/run/cri-dockerd.sock这一参数，不然可能会失败）

```shell
kubeadm join 192.168.211.131:6443 --token abcdef.0123456789abcdef \
        --discovery-token-ca-cert-hash sha256:4f6bede2b6136cc98db08a6f58727e3d00c39793dd3f97f7d011837be141161a --cri-socket unix:///var/run/cri-dockerd.sock
        
[preflight] Running pre-flight checks
	[WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```





### 安装calico

#### 方式一

在 Kubernetes 集群中安装和配置 Calico 是一个常见的步骤，用于提供网络连接和网络安全策略。Calico 既可以作为网络插件，也可以提供网络策略引擎。以下是安装 Calico 的步骤：

##### 1. 准备 Kubernetes 集群

确保你已经有一个运行中的 Kubernetes 集群，并且可以使用 `kubectl` 命令行工具与其交互。

##### 2. 安装 Calico

你可以使用 `kubectl apply` 命令从 Calico 官方提供的 YAML 文件中安装 Calico。

##### 2.1 下载 Calico 配置文件

你可以直接应用 Calico 官方的安装文件。以下命令将安装 Calico 网络插件：

```shell
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
```

这个 YAML 文件包含了所有必要的资源，如 DaemonSet、ClusterRole、ServiceAccount 等。

##### 2.2 验证安装

确保所有的 Calico Pod 都在运行：

```shell
kubectl get pods -n kube-system
```

你应该看到类似于以下输出，所有 Calico 相关的 Pod 都应该是 `Running` 状态：

```shell
NAME                                      READY   STATUS    RESTARTS   AGE
calico-kube-controllers-847c8c99d8-7n4t8  1/1     Running   0          5m
calico-node-hp8lq                         1/1     Running   0          5m
calico-node-j9qk4                         1/1     Running   0          5m
calico-node-lkh0d                         1/1     Running   0          5m
calico-typha-79b4ff76fb-k2rml             1/1     Running   0          5m
calico-typha-79b4ff76fb-q9lq5             1/1     Running   0          5m
```

##### 3. 配置 Calico 网络策略（可选）

Calico 不仅提供网络连接，还提供丰富的网络策略功能。你可以创建网络策略来控制 Pod 之间的流量。

##### 3.1 示例网络策略

以下是一个示例网络策略，允许所有命名空间内的 Pod 之间的流量，并阻止所有其他流量：

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all
  namespace: default
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector: {}
  egress:
  - to:
    - podSelector: {}
```

应用这个策略：

```yaml
kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all
  namespace: default
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector: {}
  egress:
  - to:
    - podSelector: {}
EOF
```

##### 4. 配置 Calico 节点（可选）

如果需要进一步配置 Calico 节点，例如指定 IP 池或修改 IPAM 配置，你可以编辑 Calico 的 ConfigMap。

##### 4.1 编辑 ConfigMap

编辑 `calico-config` ConfigMap：

```shell
kubectl edit cm calico-config -n kube-system
```

根据需要修改配置项，例如 `calico_backend`、`veth_mtu` 等。

##### 5. 验证网络连接

使用简单的 Pod 测试网络连接：

```shell
kubectl run test-nginx --image=nginx --restart=Never
kubectl get pods -o wide
```

验证 Pod 是否能互相通信：

```shell
kubectl exec -it test-nginx -- ping <another-pod-ip>
```

通过以上步骤，你就可以在 Kubernetes 集群中安装和配置 Calico，提供高效的网络连接和安全策略功能。



#### 方式二



GitHub：https://github.com/projectcalico/calico

官方文档：https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart

或者使用网络插件`calico`：https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart

```shell
git clone https://github.com/projectcalico/calico.git

# calico.yaml在目录/root/calico-master/manifests 内

#执行创建calico

root@k8s-master01:~#  kubectl apply -f /root/calico-master/manifests/calico.yaml
poddisruptionbudget.policy/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
serviceaccount/calico-node created
serviceaccount/calico-cni-plugin created
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrole.rbac.authorization.k8s.io/calico-cni-plugin created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-cni-plugin created
daemonset.apps/calico-node created
deployment.apps/calico-kube-controllers created

```

等待它安装完毕稍等片刻， 发现集群的状态已经是Ready

```shell
root@k8s-master01:~# kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
k8s-master01   Ready    control-plane   29m   v1.30.1
k8s-node01     Ready    <none>          27m   v1.30.1
k8s-node02     Ready    <none>          27m   v1.30.1

```

