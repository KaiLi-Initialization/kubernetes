参考文档：https://blog.csdn.net/SeeYouGoodBye/article/details/135706243

# 环境初始化

## 1 检查操作系统的版本

```powershell
# 此方式下安装kubernetes集群系统版本为：Ubuntu 24.04 LTS 

root@k8s-master01:~# uname -a
Linux k8s-master01 6.8.0-31-generic #31-Ubuntu SMP PREEMPT_DYNAMIC Sat Apr 20 00:40:06 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

## 2 主机名解析

为了方便集群节点间的直接调用，在这个配置一下主机名解析，企业中推荐使用内部DNS服务器

```powershell
修改主机名使用：hostnamectl set-hostname 主机名 

# 主机名成解析 编辑三台服务器的/etc/hosts文件，添加下面内容

cat >> /etc/hosts <<EOF
43.163.245.36  ubuntu-master
43.153.184.115 ubuntu-node01
43.153.135.59  ubuntu-node02
EOF
```

## 关闭ufw

```shell
systemctl stop ufw 
systemctl disable ufw 
```



## 6 禁用swap分区

swap分区指的是虚拟内存分区，它的作用是物理内存使用完，之后将磁盘空间虚拟成内存来使用，启用swap设备会对系统的性能产生非常负面的影响，因此kubernetes要求每个节点都要禁用swap设备，但是如果因为某些原因确实不能关闭swap分区，就需要在集群安装过程中通过明确的参数进行配置说明

```powershell
方式一：重新启动电脑，永久禁用Swap
# 编辑分区配置文件/etc/fstab，注释掉swap分区一行
# 注意修改完毕之后需要重启linux服务
vim /etc/fstab
注释掉 /swap.img 这一行
# /swap.img      none    swap    sw      0       0


方式二：不重启电脑，禁用启用swap，立刻生效
swapoff -a 禁用命令
swapon -a  启用命令
free -mh   查看交换分区的状态:
```

或运行一下命令

```shell
$ sudo swapoff -a 
$ sudo sed -i '/ swap / s / ^ \ (.* \ ) $ /# \ 1 / g' /etc/fstab
```



## 7 修改linux的内核参数（待确认）

管理员可以通过 sysctl 接口修改内核运行时的参数,在 `/proc/sys/` 虚拟文件系统下存放许多内核参数。这些参数涉及了多个内核子系统，如：

- 内核子系统（通常前缀为: `kernel.`）
- 网络子系统（通常前缀为: `net.`）
- 虚拟内存子系统（通常前缀为: `vm.`）

若要获取完整的参数列表，请执行以下命令：

```shell
sudo sysctl -a
```

* net.bridge.bridge-nf-call-iptables：开启桥设备内核监控（ipv4）
* net.ipv4.ip_forward：开启路由转发
* net.bridge.bridge-nf-call-ip6tables：开启桥设备内核监控（ipv6）



```shell
内核低于 4.1 版本需要添加 fs.may_detach_mounts=1 和 net.ipv4.tcp_tw_recycle=0

在内核低于 4.1 中，不要设置 net.ipv4.tcp_tw_recycle 这个参数为 1
，网上有不少教程没提到或者内核版本过低系统默认设置为 1
开启此参数，对于外网的 sockets 链接会快速回收。但是对于内网会导致大量的 TCP 链接建立错误。k8s
使用的都是在内网，所以要禁用！设置为 0
有关 net.ipv4.tcp_tw_recycle 参数查看文章(https://cloud.tencent.com/developer/article/1683704)

参数 fs.may_detach_mounts 则是跟容器相关的。该参数如果设置为 0，会导致服务变更后旧 pod
在回收时会一直卡在 Terminating 的状态，会重复出现 UnmountVolume.TearDown failed for volume 错误。
有关 fs.may_detach_mounts 参数产生的 bug 查看文章(https://github.com/kubernetes/kubernetes/issues/51835)
以及 (https://bugzilla.redhat.com/show_bug.cgi?id=1441737) 。
```

以上3项为必须参数，其他参数可根据需要添加

```powershell
# 修改linux的内核采纳数，添加网桥过滤和地址转发功能
# 编辑/etc/sysctl.d/kubernetes.conf文件，添加如下配置：

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# 设置所需的 sysctl 参数，参数在重新启动后保持不变

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# 应用 sysctl 参数而不重新启动
sudo sysctl --system

# 重新加载配置
sysctl -p
# 加载网桥过滤模块

modprobe br_netfilter
modprobe overlay

# 查看网桥过滤模块是否加载成功
[root@master ~]# lsmod | grep br_netfilter & lsmod | grep overlay

```

通过运行以下指令确认 `net.bridge.bridge-nf-call-iptables`、`net.bridge.bridge-nf-call-ip6tables` 和 `net.ipv4.ip_forward` 系统变量在你的 `sysctl` 配置中被设置为 1：

```bash
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward
```

### 启用 IPv4 数据包转发

参考文档：https://kubernetes.io/docs/setup/production-environment/container-runtimes/

要手动启用 IPv4 数据包转发：

```bash
# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system
```

使用以下命令验证是否`net.ipv4.ip_forward`设置为 1：

```bash
sysctl net.ipv4.ip_forward
```



# 安装containerd

安装containerd依赖包

```shell
sudo apt install -y curl gnupg2 software-properties-common apt-transport-https ca-certificates
```

启用 docker 仓库

```shell
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmour -o /etc/apt/trusted.gpg.d/docker.gpg 

sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
```

现在，运行以下 apt 命令来安装 containerd

```shell
sudo apt update 
sudo apt install -y containerd.io
```

配置 containerd 以便它开始使用 systemd 作为 cgroup。

Containerd 的默认配置文件为 `/etc/containerd/config.toml`

```shell
containerd config default | sudo tee /etc/containerd/config.toml >/dev/null 2>&1 

sudo sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml
```





**参考文档：**https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#docker

结合 `runc` 使用 `systemd` cgroup 驱动，在 `/etc/containerd/config.toml` 中设置：

```shell
vim /etc/containerd/config.toml

# 找到containerd.runtimes.runc.options，添加SystemdCgroup = true

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes]

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          base_runtime_spec = ""
          cni_conf_dir = ""
          cni_max_conf_num = 0
          container_annotations = []
          pod_annotations = []
          privileged_without_host_devices = false
          runtime_engine = ""
          runtime_path = ""
          runtime_root = ""
          runtime_type = "io.containerd.runc.v2"

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = "true"    # 此处修改为true
            SystemdCgroup = false
```

或者运行以下命令

```shell
sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml
```



**替换镜像源**

```shell
# 所有节点将sandbox_image的Pause镜像改成符合自己版本的地址：registry.aliyuncs.com/google_containers/pause:3.9
$ vim /etc/containerd/config.toml

  [plugins."io.containerd.grpc.v1.cri"]
    device_ownership_from_security_context = false
    disable_apparmor = false
    disable_cgroup = false
    disable_hugetlb_controller = true
    disable_proc_mount = false
    disable_tcp_service = true
    enable_selinux = false
    enable_tls_streaming = false
    enable_unprivileged_icmp = false
    enable_unprivileged_ports = false
    ignore_image_defined_volumes = false
    max_concurrent_downloads = 3
    max_container_log_line_size = 16384
    netns_mounts_under_state_dir = false
    restrict_oom_score_adj = false
    sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.9" # 此处更换
    
#等同于：
$ sed -i "s#registry.k8s.io/pause#registry.aliyuncs.com/google_containers/pause#g"  /etc/containerd/config.toml
```



设置开机启动

```shell
systemctl daemon-reload
systemctl enable --now containerd
systemctl status containerd.service
containerd  --version    #查看版本
```



# 安装Kubernetes组件

以下指令适用于 Kubernetes 1.31.

1. 更新 `apt` 包索引并安装使用 Kubernetes `apt` 仓库所需要的包：

   ```shell
   sudo apt-get update
   # apt-transport-https 可能是一个虚拟包（dummy package）；如果是的话，你可以跳过安装这个包
   sudo apt-get install -y apt-transport-https ca-certificates curl gpg
   ```

2. 下载用于 Kubernetes 软件包仓库的公共签名密钥。所有仓库都使用相同的签名密钥，因此你可以忽略URL中的版本：

   ```shell
   # 如果 `/etc/apt/keyrings` 目录不存在，则应在 curl 命令之前创建它，请阅读下面的注释。
   # sudo mkdir -p -m 755 /etc/apt/keyrings
   curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
   ```

   说明：

   在低于 Debian 12 和 Ubuntu 22.04 的发行版本中，`/etc/apt/keyrings` 默认不存在。 应在 curl 命令之前创建它。

3. 添加 Kubernetes `apt` 仓库。 请注意，此仓库仅包含适用于 Kubernetes 1.31 的软件包； 对于其他 Kubernetes 次要版本，则需要更改 URL 中的 Kubernetes 次要版本以匹配你所需的次要版本 （你还应该检查正在阅读的安装文档是否为你计划安装的 Kubernetes 版本的文档）。

   ```shell
   # 此操作会覆盖 /etc/apt/sources.list.d/kubernetes.list 中现存的所有配置。
   echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
   ```

   

4. 更新 `apt` 包索引，安装 kubelet、kubeadm 和 kubectl，并锁定其版本：

   ```shell
   sudo apt-get update
   sudo apt-get install -y kubelet kubeadm kubectl
   sudo apt-mark hold kubelet kubeadm kubectl
   ```

​      kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环。

5. 配置kubelet的cgroup
   ```shell
   #编辑/etc/sysconfig/kubelet, 添加下面的配置
   
   cat <<EOF | sudo tee /etc/sysconfig/kubelet
   KUBELET_CGROUP_ARGS="--cgroup-driver=systemd"
   EOF
   ```

   

6. 设置kubelet开机自启

   ```shell
   systemctl enable kubelet
   ```

   kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环。



# 集群初始化

现在，我们已准备好初始化 Kubernetes 集群。仅在主节点上运行以下 Kubeadm 命令。

```shell
sudo kubeadm init --control-plane-endpoint=ubuntu-master
```

通过配置文件`kubeadm.yaml`初始化集群

```shell
root@ubuntu-master:/home/ubuntu# sudo kubeadm init --control-plane-endpoint=ubuntu-master
I1113 14:21:13.900423    9652 version.go:256] remote version is much newer: v1.31.2; falling back to: stable-1.28
[init] Using Kubernetes version: v1.28.15
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W1113 14:21:27.026013    9652 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.8" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local ubuntu-master] and IPs [10.96.0.1 10.7.4.2]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost ubuntu-master] and IPs [10.7.4.2 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost ubuntu-master] and IPs [10.7.4.2 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 6.001739 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node ubuntu-master as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node ubuntu-master as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: 1h8yll.bvfejwy16lo240fz
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join ubuntu-master:6443 --token 1h8yll.bvfejwy16lo240fz \
        --discovery-token-ca-cert-hash sha256:8adfe241a90833c26e902272c7ea166856bfb33a23dc10b51221154297a8a4ae \
        --control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join ubuntu-master:6443 --token 1h8yll.bvfejwy16lo240fz \
        --discovery-token-ca-cert-hash sha256:8adfe241a90833c26e902272c7ea166856bfb33a23dc10b51221154297a8a4ae 
```

如果是高可用，其他master节点运行如下

```shell
kubeadm join 192.168.211.131:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:c6c6fed04542f5c55bd75362e28f5691710b8c772bcfcde024850bef176caeb0 --cri-socket unix:///var/run/cri-dockerd.sock

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```



重置再初始化

```shell
kubeadm reset --cri-socket unix:///var/run/cri-dockerd.sock

rm -fr ~/.kube/  /etc/kubernetes/* var/lib/etcd/*
```



> 下面的操作只需要在node节点上执行即可（node节点加入集群）

```powershell
# 根据提示把worker节点加进master节点，复制你们各自在日志里的提示，然后分别粘贴在2个worker节点上，最后回车即可（注意要在后面加上--cri-socket unix:///var/run/cri-dockerd.sock这一参数，不然可能会失败）

以node01节点为例：
[root@node01 ~]# kubeadm join 192.168.211.131:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:c6c6fed04542f5c55bd75362e28f5691710b8c772bcfcde024850bef176caeb0 --cri-socket unix:///var/run/cri-dockerd.sock
[preflight] Running pre-flight checks
	[WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```

在master上查看节点信息

```powershell
[root@master ~]# kubectl get nodes
```



# 安装calico

GitHub：https://github.com/projectcalico/calico

官方文档：https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart

或者使用网络插件`calico`：https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart

需要网络插件才能实现集群中 Pod 之间的通信。运行以下 kubectl 命令从主节点安装 Calico 网络插件，

```
$ kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.0/manifests/calico.yaml
```





# 新节点加入

查看当前token

默认情况下，节点加入令牌会在 24 小时后过期。

```shell
root@ubuntu-master:/home/ubuntu# kubeadm token list

TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
o89ll6.j1nqs5g079numh98   5h          2024-11-14T09:04:48Z   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token
```

生成新token

```shell
root@ubuntu-master:/home/ubuntu# sudo kubeadm token create

o89ll6.j1nqs5g079numh98
```

生成CA Key

如果你没有 `--discovery-token-ca-cert-hash` 的具体值，可以在控制平面节点上运行以下命令来获取：

```bash
# 在控制平面节点上运行此命令
root@ubuntu-master:/home/ubuntu# sudo cat /etc/kubernetes/pki/ca.crt | openssl x509 -pubkey  | openssl rsa -pubin -outform der 2>/dev/null | \
   openssl dgst -sha256 -hex | sed 's/^.* //'
   
4ac9f8430aab71080ecdb8973ce1459a27cb3e3e6bcd609599793354b23ac27f
```

增加节点

kubeadm init格式：

```shell
sudo kubeadm join --token <token> <control-plane-host>:<control-plane-port> --discovery-token-ca-cert-hash sha256:<hash>


sudo kubeadm join 10.7.0.9:6443 --token ug9muf.0l1fst13gotetl6z --discovery-token-ca-cert-hash sha256:a97835357aab18a3fddc1e3fe525668216ca532cf556ea46133b6b31e47f2833 --cri-socket unix:///var/run/cri-dockerd.sock
```



工作节点加入集群

```shell
kubeadm join ubuntu-master:6443 --token o89ll6.j1nqs5g079numh98 --discovery-token-ca-cert-hash sha256:4ac9f8430aab71080ecdb8973ce1459a27cb3e3e6bcd609599793354b23ac27f
```

控制节点加入集群

```shell
kubeadm join ubuntu-master:6443 --token o89ll6.j1nqs5g079numh98 --discovery-token-ca-cert-hash sha256:4ac9f8430aab71080ecdb8973ce1459a27cb3e3e6bcd609599793354b23ac27f --control-plane
```

