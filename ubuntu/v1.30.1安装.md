参考文档：https://blog.csdn.net/SeeYouGoodBye/article/details/135706243

# 环境初始化

## 1 检查操作系统的版本

```powershell
# 此方式下安装kubernetes集群系统版本为：Ubuntu 24.04 LTS 

root@k8s-master01:~# uname -a
Linux k8s-master01 6.8.0-31-generic #31-Ubuntu SMP PREEMPT_DYNAMIC Sat Apr 20 00:40:06 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

## 2 主机名解析

为了方便集群节点间的直接调用，在这个配置一下主机名解析，企业中推荐使用内部DNS服务器

```powershell
修改主机名使用：hostnamectl set-hostname 主机名 

# 主机名成解析 编辑三台服务器的/etc/hosts文件，添加下面内容

cat >> /etc/hosts <<EOF
43.163.216.231  ubuntu-master
43.163.245.36  ubuntu-node01
10.7.0.15  ubuntu-node02
43.153.135.59 ubuntu-node02
EOF
```

## 关闭ufw

```shell
systemctl stop ufw 
systemctl disable ufw 
```



## 6 禁用swap分区

swap分区指的是虚拟内存分区，它的作用是物理内存使用完，之后将磁盘空间虚拟成内存来使用，启用swap设备会对系统的性能产生非常负面的影响，因此kubernetes要求每个节点都要禁用swap设备，但是如果因为某些原因确实不能关闭swap分区，就需要在集群安装过程中通过明确的参数进行配置说明

```powershell
方式一：重新启动电脑，永久禁用Swap
# 编辑分区配置文件/etc/fstab，注释掉swap分区一行
# 注意修改完毕之后需要重启linux服务
vim /etc/fstab
注释掉 /swap.img 这一行
# /swap.img      none    swap    sw      0       0


方式二：不重启电脑，禁用启用swap，立刻生效
swapoff -a 禁用命令
swapon -a  启用命令
free -mh   查看交换分区的状态:
```



### 启用 IPv4 数据包转发

参考文档：https://kubernetes.io/docs/setup/production-environment/container-runtimes/

要手动启用 IPv4 数据包转发：

```bash
# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system
```

使用以下命令验证是否`net.ipv4.ip_forward`设置为 1：

```bash
sysctl net.ipv4.ip_forward
```



## 9 设置系统时区与同步

设置时区

```shell
timedatectl set-timezone Asia/ShanghaiCOPY
```

同步时区

```shell
systemctl enable chronyd && \
systemctl start chronyd
```

查看

```shell
timedatectl status
```

*显示如下*

>`Time zone: Asia/Shanghai (CST, +0800)` 表示使用东八区时区
>`System clock synchronized: yes` 表示时区有同步
>`NTP service: active` 表示开启了时区同步服务

```text
               Local time: 三 2022-09-21 01:12:09 CST
           Universal time: 二 2022-09-20 17:12:09 UTC
                 RTC time: 二 2022-09-20 17:12:09
                Time zone: Asia/Shanghai (CST, +0800)
System clock synchronized: yes
              NTP service: active
          RTC in local TZ: no
```

将当前的 UTC 时间写入硬件时钟

```shell
timedatectl set-local-rtc 0
```

重启依赖于系统时间的服务

```shell
systemctl restart rsyslog && \
systemctl restart crond
```





# 二进制安装containerd

containerd已包含Kubernetes CRI 功能，和docker不同的是无需下载`cri-containerd-....`档案即可使用 CRI。

二进制安装containerd需要**`runc`**和**`CNI`**插件的支持。

#### 第 1 步：安装containerd

参考文档：https://github.com/containerd/containerd/blob/main/docs/getting-started.md

下载地址：https://containerd.io/downloads/

所有设备安装（`master`和`node`节点）

containerd 的官方二次制作版本可用于`amd64`（也称为`x86_64`）和`arm64`（也称为`aarch64`）结构。

[从https://github.com/containerd/containerd/releases](https://github.com/containerd/containerd/releases)下载`containerd-<VERSION>-<OS>-<ARCH>.tar.gz`存档，验证其sha256sum，并将其解压到：`/usr/local`

```shell
[root@master ~]# wget https://github.com/containerd/containerd/releases/download/v1.7.17/containerd-1.7.17-linux-amd64.tar.gz

[root@master ~]# tar Cxzvf /usr/local containerd-1.7.17-linux-amd64.tar.gz
bin/
bin/ctr
bin/containerd-stress
bin/containerd
bin/containerd-shim
bin/containerd-shim-runc-v1
bin/containerd-shim-runc-v2


# 文件说明
containerd 的安装包中一共有五个文件，通过上面的命令它们被安装到了 usr/local/bin 目录中：

1. containerd：即容器的运行时，以 gRPC 协议的形式提供满足 OCI 标准的 API

2. containerd-release：containerd 项目的发行版发布工具

3. containerd-stress：containerd压力测试工具

4. containerd-shim：这是每一个容器的运行时载体，我们在 docker 宿主机上看到的 shim 也正是代表着一个个通过调用 containerd 启动的 docker 容器。

5. ctr：它是一个简单的 CLI 接口，用作 containerd 本身的一些调试用途，投入生产使用时还是应该配合docker 或者 cri-containerd 部署。
```

##### 1）修改containerd配置

配置Containerd所需的模块（所有节点）

```shell
cat <<EOF> /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

# 所有节点加载模块：
modprobe -- overlay
modprobe -- br_netfilter
```

创建初始配置文件

Containerd 的默认配置文件为 `/etc/containerd/config.toml`

```shell
$ mkdir -p /etc/containerd/
$ containerd config default > /etc/containerd/config.toml    #创建默认的配置文件
```



##### 2）配置 `systemd` cgroup 驱动

**参考文档：**https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#docker

结合 `runc` 使用 `systemd` cgroup 驱动，在 `/etc/containerd/config.toml` 中设置：

```shell
vim /etc/containerd/config.toml

# 找到containerd.runtimes.runc.options，添加SystemdCgroup = true

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes]

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          base_runtime_spec = ""
          cni_conf_dir = ""
          cni_max_conf_num = 0
          container_annotations = []
          pod_annotations = []
          privileged_without_host_devices = false
          runtime_engine = ""
          runtime_path = ""
          runtime_root = ""
          runtime_type = "io.containerd.runc.v2"

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = "true"    # 此处修改为true
            SystemdCgroup = false
```

或者运行以下命令

```shell
sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml
```

#### 重载沙箱（pause）镜像

在你的 [containerd 配置](https://github.com/containerd/containerd/blob/main/docs/cri/config.md)中， 你可以通过设置以下选项重载沙箱镜像：

```toml
[plugins."io.containerd.grpc.v1.cri"]
  sandbox_image = "registry.k8s.io/pause:3.10"   # 需要查看当前kubernetes版本所用的pause版本
```

一旦你更新了这个配置文件，可能就同样需要重启 `containerd`：`systemctl restart containerd`。

**替换镜像源**

```shell
# 所有节点将sandbox_image的Pause镜像改成符合自己版本的地址：registry.aliyuncs.com/google_containers/pause:3.9
$ vim /etc/containerd/config.toml

  [plugins."io.containerd.grpc.v1.cri"]
    device_ownership_from_security_context = false
    disable_apparmor = false
    disable_cgroup = false
    disable_hugetlb_controller = true
    disable_proc_mount = false
    disable_tcp_service = true
    enable_selinux = false
    enable_tls_streaming = false
    enable_unprivileged_icmp = false
    enable_unprivileged_ports = false
    ignore_image_defined_volumes = false
    max_concurrent_downloads = 3
    max_container_log_line_size = 16384
    netns_mounts_under_state_dir = false
    restrict_oom_score_adj = false
    sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.9" # 此处更换
    
#等同于：
$ sed -i "s#registry.k8s.io/pause#registry.aliyuncs.com/google_containers/pause#g"  /etc/containerd/config.toml
```



##### 3）通过 systemd 启动 containerd

还应该从https://raw.githubusercontent.com/containerd/containerd/main/containerd.service下载 `containerd.service`文件到目录`/usr/local/lib/systemd/system/containerd.service`(目录/usr/local/lib/systemd/system/需要创建)或`/usr/lib/systemd/system/containerd.service`中，并运行以下命令：

```shell
wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service
mkdir -p /usr/local/lib/systemd/system/
cp containerd.service /usr/local/lib/systemd/system/
```

若无法下载可创建containerd.service将下文复制进去

```shell
vim /usr/lib/systemd/system/containerd.service

# Copyright The containerd Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target local-fs.target

[Service]
#uncomment to fallback to legacy CRI plugin implementation with podsandbox support.
#Environment="DISABLE_CRI_SANDBOXES=1"
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/containerd

Type=notify
Delegate=yes
KillMode=process
Restart=always
RestartSec=5
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity
LimitNOFILE=infinity
# Comment TasksMax if your systemd version does not supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
OOMScoreAdjust=-999

[Install]
WantedBy=multi-user.target
```

设置开机启动

```shell
$ systemctl daemon-reload
$ systemctl enable --now containerd
$ systemctl status containerd.service
$ containerd  --version    #查看版本
```



#### 第 2 步：安装 runc

[从https://github.com/opencontainers/runc/releases](https://github.com/opencontainers/runc/releases)下载`runc.<ARCH>`二进制文件，验证其 sha256sum，

并将其安装到目录.`/usr/local/sbin/runc`

此处安装版本为：`v1.1.4/runc.amd64`

```shell
[root@master ~]# wget https://github.com/opencontainers/runc/releases/download/v1.1.4/runc.amd64


[root@master ~]# install -m 755 runc.amd64 /usr/local/sbin/runc
#出现以下说明安装没有问题
[root@master ~]# runc -version
runc version 1.1.4
commit: v1.1.4-0-g5fd4c4d1
spec: 1.0.2-dev
go: go1.17.10
libseccomp: 2.5.4

```

该二进制文件是静态构建的，应该适用于任何 Linux 发行版。

#### 第 3 步：安装CNI插件

[从https://github.com/containernetworking/plugins/releases](https://github.com/containernetworking/plugins/releases)下载`cni-plugins-<OS>-<ARCH>-<VERSION>.tgz`存档，验证其 sha256sum，并将其解压到：`/opt/cni/bin`

```shell
[root@master ~]# wget https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz
[root@master ~]# mkdir -p /opt/cni/bin
[root@master ~]# tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.3.0.tgz
./
./loopback
./bandwidth
./ptp
./vlan
./host-device
./tuning
./vrf
./sbr
./tap
./dhcp
./static
./firewall
./macvlan
./dummy
./bridge
./ipvlan
./portmap
./host-local
```





# 安装docker

1. 在安装 Docker Engine 之前，您需要卸载所有有冲突的软件包。

   ```shell
   for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done
   ```

   `apt-get`可能会报告您没有安装这些包。

   卸载 Docker 时，存储在中的图像、容器、卷和网络`/var/lib/docker/`不会自动删除。

   

   安装Docker Engine

   在新的主机上首次安装 Docker Engine 之前，您需要设置 Docker 存储库。之后，您可以从存储库安装和更新 Docker。

   1. 设置 Docker 的`apt`存储库。

      ```shell
      # 添加Docker官方的GPG密钥
      sudo apt-get update
      sudo apt-get install ca-certificates curl
      sudo install -m 0755 -d /etc/apt/keyrings
      sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
      sudo chmod a+r /etc/apt/keyrings/docker.asc
      
      # 将存储库添加到apt源：
      echo \
        "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
        $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
        sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
      sudo apt-get update
      ```

      

   2. 安装最新版本docker-ce

   ```shell
   sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
   
   # 过运行映像来验证 Docker Engine 安装是否成功 hello-world。
   
   sudo docker run hello-world
   ```

2. 使用 `systemd` 驱动（未配置）

   **参考文档：**https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#cgroupfs-cgroup-driver

    由于 kubeadm 把 kubelet 视为一个 [系统服务](https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/kubelet-integration)来管理， 所以对基于 kubeadm 的安装， 我们推荐使用 `systemd` 驱动， 不推荐 kubelet [默认](https://kubernetes.io/zh-cn/docs/reference/config-api/kubelet-config.v1beta1)的 `cgroupfs` 驱动。

   

   Docker 在默认情况下使用Vgroup Driver为cgroupfs，而Kubernetes推荐使用systemd来替代cgroupfs

   

   ```shell
   mkdir -p /etc/docker
   
   cat <<EOF> /etc/docker/daemon.json
   {
   	"exec-opts": ["native.cgroupdriver=systemd"]      
   }
   EOF
   ```

   

   

   

## 卸载 Docker Engine



1. 卸载 Docker Engine、CLI、containerd 和 Docker Compose 软件包：

   ```console
   sudo yum remove docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras
   ```

2. 主机上的镜像、容器、卷或自定义配置文件不会自动删除。要删除所有镜像、容器和卷，请执行以下操作：

   

   ```console
   sudo rm -rf /var/lib/docker
   sudo rm -rf /var/lib/containerd
   ```

您必须手动删除任何已编辑的配置文件。



##  cri-dockerd安装

kubernetes1.26.3版本需要安装cri-dockerd作为网络接口

**参考文档：**https://github.com/mirantis/cri-dockerd#build-and-install

[cri-dockerd下载地址](https://github.com/Mirantis/cri-dockerd/releases)

下载软件包

```shell
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.14/cri-dockerd-0.3.14.amd64.tgz
```

安装cri-dockerd

```shell
tar -xf cri-dockerd-0.3.14.amd64.tgz

cd cri-dockerd

mkdir -p /usr/local/bin

install -o root -g root -m 0755 cri-dockerd /usr/local/bin/cri-dockerd

```



编写service

```shell
vim /etc/systemd/system/cri-docker.service

[Unit]
Description=CRI Interface for Docker Application Container Engine
Documentation=https://docs.mirantis.com
After=network-online.target firewalld.service docker.service
Wants=network-online.target
Requires=cri-docker.socket
 
[Service]
Type=notify
ExecStart=/usr/local/bin/cri-dockerd --container-runtime-endpoint fd:// --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9
ExecReload=/bin/kill -s HUP $MAINPID
TimeoutSec=0
RestartSec=2
Restart=always
 
# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
# Both the old, and new location are accepted by systemd 229 and up, so using the old location
# to make them work for either version of systemd.
StartLimitBurst=3
 
# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
# this option work for either version of systemd.
StartLimitInterval=60s


# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
 
# Comment TasksMax if your systemd version does not support it.
# Only systemd 226 and above support this option.
TasksMax=infinity
Delegate=yes
KillMode=process
 
[Install]
WantedBy=multi-user.target
```

编写socket

```shell

vim /etc/systemd/system/cri-docker.socket

[Unit]
Description=CRI Docker Socket for the API
PartOf=cri-docker.service
 
[Socket]
ListenStream=%t/cri-dockerd.sock
SocketMode=0660
SocketUser=root
SocketGroup=docker
 
[Install]
WantedBy=sockets.target

```

启用cri-docker

```shell
systemctl daemon-reload
systemctl start cri-docker
systemctl enable cri-docker.service
systemctl enable --now cri-docker.socket

systemctl status cri-docker
```



**Kubernetes使用**（安装组件后配置）(待验证)

1. Kubernetes网络插件默认是cni，需要追加**`--network-plugin=cni`**，通过该配置告诉容器，使用kubernetes的网络接口。
2. pause镜像是一切的 Pod 的基础

```shell
# go编译安装路径
vim /etc/systemd/system/cri-docker.service
# rpm安装路径
vim /usr/lib/systemd/system/cri-docker.service

[root@master ~]# cat /etc/systemd/system/cri-docker.service 
[Unit]
Description=CRI Interface for Docker Application Container Engine
Documentation=https://docs.mirantis.com
After=network-online.target firewalld.service docker.service
Wants=network-online.target
Requires=cri-docker.socket

[Service]
Type=notify
ExecStart=/usr/local/bin/cri-dockerd --container-runtime-endpoint fd:// # 此处添加一下内容
ExecReload=/bin/kill -s HUP $MAINPID
TimeoutSec=0
RestartSec=2
Restart=always

# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
# Both the old, and new location are accepted by systemd 229 and up, so using the old location
# to make them work for either version of systemd.
StartLimitBurst=3

# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
# this option work for either version of systemd.
StartLimitInterval=60s

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Comment TasksMax if your systemd version does not support it.
# Only systemd 226 and above support this option.
TasksMax=infinity
Delegate=yes
KillMode=process

[Install]
WantedBy=multi-user.target

```

需要将以上两条1，2的参数追加在ExecStart后面

```shell
ExecStart=/usr/local/bin/cri-dockerd --container-runtime-endpoint fd:// --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9
```

**启动cri-dockerd**

```shell
systemctl daemon-reload
systemctl enable cri-docker.service
systemctl enable --now cri-docker.socket
systemctl start cri-docker
```

验证cri-dockerd

```shell
[root@master ~]# systemctl status cri-docker
● cri-docker.service - CRI Interface for Docker Application Container Engine
     Loaded: loaded (/etc/systemd/system/cri-docker.service; enabled; vendor preset: disabled)
     Active: active (running) since Tue 2022-12-06 23:31:18 CST; 1min 34s ago
TriggeredBy: ● cri-docker.socket
       Docs: https://docs.mirantis.com
   Main PID: 6861 (cri-dockerd)
      Tasks: 6
     Memory: 50.3M
        CPU: 84ms
     CGroup: /system.slice/cri-docker.service
             └─6861 /usr/local/bin/cri-dockerd --container-runtime-endpoint fd:// --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9

Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Start docker client with request timeout 0s"
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Hairpin mode is set to none"
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Loaded network plugin cni"
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Docker cri networking managed by network plugin cni"
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Docker Info: &{ID:37LI:R2SF:5ZQM:SFBX:6KGE:KFLT:CWY5:ZF4D:LK2C:GSBR:DBJH:K5SA Contain>
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Setting cgroupDriver systemd"
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCid>
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Dec 06 23:31:18 master cri-dockerd[6861]: time="2022-12-06T23:31:18+08:00" level=info msg="Start cri-dockerd grpc backend"
Dec 06 23:31:18 master systemd[1]: Started CRI Interface for Docker Application Container Engine.
lines 1-22/22 (END)

```

表示安装完成。

# 安装Kubernetes组件

以下指令适用于 Kubernetes 1.30.

1. 更新 `apt` 包索引并安装使用 Kubernetes `apt` 仓库所需要的包：

   ```shell
   sudo apt-get update
   # apt-transport-https 可能是一个虚拟包（dummy package）；如果是的话，你可以跳过安装这个包
   sudo apt-get install -y apt-transport-https ca-certificates curl gpg
   ```

2. 下载用于 Kubernetes 软件包仓库的公共签名密钥。所有仓库都使用相同的签名密钥，因此你可以忽略URL中的版本：

   ```shell
   #说明：
   在低于 Debian 12 和 Ubuntu 22.04 的发行版本中，`/etc/apt/keyrings` 默认不存在。 应在 curl 命令之前创建它。
   # sudo mkdir -p -m 755 /etc/apt/keyrings
   
   curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
   ```

3. 添加 Kubernetes `apt` 仓库。 请注意，此仓库仅包含适用于 Kubernetes 1.30 的软件包； 对于其他 Kubernetes 次要版本，则需要更改 URL 中的 Kubernetes 次要版本以匹配你所需的次要版本 （你还应该检查正在阅读的安装文档是否为你计划安装的 Kubernetes 版本的文档）。

   ```shell
   # 此操作会覆盖 /etc/apt/sources.list.d/kubernetes.list 中现存的所有配置。
   
       echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
   ```

   

4. 更新 `apt` 包索引，安装 kubelet、kubeadm 和 kubectl，并锁定其版本：

   ```shell
   sudo apt-get update
   sudo apt-get install -y kubelet kubeadm kubectl
   sudo apt-mark hold kubelet kubeadm kubectl
   ```

   

5. 配置kubelet的cgroup驱动

   通过kubeadm init初始化集群时进行设置，kubeadm-config.yaml文件加入以下内容

   ```shell
   # kubeadm-config.yaml
   kind: ClusterConfiguration
   apiVersion: kubeadm.k8s.io/v1beta4
   kubernetesVersion: v1.21.0
   ---
   kind: KubeletConfiguration
   apiVersion: kubelet.config.k8s.io/v1beta1
   cgroupDriver: systemd
   ```

   通过手动配置文件方式

   ```shell
   #编辑/etc/sysconfig/kubelet, 添加下面的配置
   
   cat <<EOF | sudo tee /etc/sysconfig/kubelet
   KUBELET_CGROUP_ARGS="--cgroup-driver=systemd"
   EOF
   ```

6. 设置kubelet开机自启

   ```shell
   systemctl enable kubelet
   ```

   kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环。



# 集群初始化

### 生成kubeadm初始化配置文件

```shell
[root@master ~]# kubeadm config print init-defaults
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4           # 此处修改为master节点的IP地址
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock   # 此处指定使用的container runtime的sock路径
  criSocket: unix:///var/run/cri-dockerd.sock             # 此处指定使用的docker runtime的sock路径
  imagePullPolicy: IfNotPresent
  name: node
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.k8s.io      # 此处为下载镜像的仓库地址
kind: ClusterConfiguration
kubernetesVersion: 1.26.0       # 修改为所需要的K8S版本号
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}

# 或者生成配置文件存储在文件：kubeadm.yaml中
[root@master ~]#  kubeadm config print init-defaults > kubeadm.yaml
```

###### 下载集群所需组件镜像

```shell
# 查看所需要的镜像
[root@master ~]# kubeadm config images list
registry.k8s.io/kube-apiserver:v1.28.1
registry.k8s.io/kube-controller-manager:v1.28.1
registry.k8s.io/kube-scheduler:v1.28.1
registry.k8s.io/kube-proxy:v1.28.1
registry.k8s.io/pause:3.9
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1

```

通过配置文件`kubeadm.yaml`下载kubenetes所需镜像

```shell
#直接下载
[root@centos-master ~]# kubeadm config images pull --cri-socket unix:///var/run/cri-dockerd.sock --image-repository registry.aliyuncs.com/google_containers

# 通过kubeadm.yaml文档下载
[root@centos-master ~]# kubeadm config images pull --config kubeadm.yaml

[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.9
[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.5.9-0
[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.10.1
```

通过配置文件`kubeadm.yaml`初始化集群

```shell
root@k8s-master01:~# kubeadm init --config kubeadm.yaml  --upload-certs
[init] Using Kubernetes version: v1.30.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.211.131]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master01 localhost] and IPs [192.168.211.131 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master01 localhost] and IPs [192.168.211.131 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 501.912433ms
[api-check] Waiting for a healthy API server. This can take up to 4m0s
[api-check] The API server is healthy after 3.501269747s
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
a982dc59569be26e866ff13e669969b3718e61917d008ca344d0585386851752
[mark-control-plane] Marking the node k8s-master01 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s-master01 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: abcdef.0123456789abcdef
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

# master节点运行

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

# master节点运行

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

#node节点运行

kubeadm join 192.168.211.131:6443 --token abcdef.0123456789abcdef \
        --discovery-token-ca-cert-hash sha256:c6c6fed04542f5c55bd75362e28f5691710b8c772bcfcde024850bef176caeb0
root@k8s-master01:~#

```

如果是高可用，其他master节点运行如下

```shell
kubeadm join 192.168.211.131:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:c6c6fed04542f5c55bd75362e28f5691710b8c772bcfcde024850bef176caeb0 --cri-socket unix:///var/run/cri-dockerd.sock

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```



重置再初始化

```shell
    kubeadm reset --cri-socket unix:///var/run/cri-dockerd.sock

rm -fr ~/.kube/  /etc/kubernetes/* var/lib/etcd/*
```



> 下面的操作只需要在node节点上执行即可（node节点加入集群）

```powershell
# 根据提示把worker节点加进master节点，复制你们各自在日志里的提示，然后分别粘贴在2个worker节点上，最后回车即可（注意要在后面加上--cri-socket unix:///var/run/cri-dockerd.sock这一参数，不然可能会失败）

以node01节点为例：
[root@node01 ~]# kubeadm join 192.168.211.131:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:c6c6fed04542f5c55bd75362e28f5691710b8c772bcfcde024850bef176caeb0 --cri-socket unix:///var/run/cri-dockerd.sock
[preflight] Running pre-flight checks
	[WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```

在master上查看节点信息

```powershell
[root@master ~]# kubectl get nodes
NAME    STATUS   ROLES     AGE   VERSION
master  NotReady  master   6m    v1.17.4
node1   NotReady   <none>  22s   v1.17.4
node2   NotReady   <none>  19s   v1.17.4
```

##### 安装网络插件，只在master节点操作即可



### 安装calico

GitHub：https://github.com/projectcalico/calico

官方文档：https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart

或者使用网络插件`calico`：https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart

```shell
git clone https://github.com/projectcalico/calico.git

# calico.yaml在目录/root/calico-master/manifests 内

#执行创建calico

root@k8s-master01:~#  kubectl apply -f /root/calico-master/manifests/calico.yaml
poddisruptionbudget.policy/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
serviceaccount/calico-node created
serviceaccount/calico-cni-plugin created
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgpfilters.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrole.rbac.authorization.k8s.io/calico-cni-plugin created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-cni-plugin created
daemonset.apps/calico-node created
deployment.apps/calico-kube-controllers created

```

等待它安装完毕稍等片刻， 发现集群的状态已经是Ready

```shell
root@k8s-master01:~# kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
k8s-master01   Ready    control-plane   29m   v1.30.1
k8s-node01     Ready    <none>          27m   v1.30.1
k8s-node02     Ready    <none>          27m   v1.30.1

```



### 网络插件排错

#### 故障现象

网络插件的POD无法创建：

```shell
[root@centos-master ~]# kubectl get pod -A -owide
NAMESPACE      NAME                                    READY   STATUS              RESTARTS        AGE   IP               NODE            NOMINATED NODE   READINESS GATES
kube-flannel   kube-flannel-ds-4fdp8                   0/1     CrashLoopBackOff    7 (2m29s ago)   13m   192.168.10.142   centos-node02   <none>           <none>
kube-flannel   kube-flannel-ds-5wtnh                   0/1     CrashLoopBackOff    7 (2m8s ago)    13m   192.168.10.140   centos-master   <none>           <none>
kube-flannel   kube-flannel-ds-bq6hk                   0/1     CrashLoopBackOff    7 (2m2s ago)    13m   192.168.10.141   centos-node01   <none>           <none>
kube-system    coredns-66f779496c-6qfh9                0/1     ContainerCreating   0               45m   <none>           centos-node02   <none>           <none>
kube-system    coredns-66f779496c-wcmn6                0/1     ContainerCreating   0               45m   <none>           centos-node02   <none>           <none>
kube-system    etcd-centos-master                      1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
kube-system    kube-apiserver-centos-master            1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
kube-system    kube-controller-manager-centos-master   1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
kube-system    kube-proxy-26gbn                        1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
kube-system    kube-proxy-kjmfp                        1/1     Running             2 (23m ago)     44m   192.168.10.142   centos-node02   <none>           <none>
kube-system    kube-proxy-n9hr4                        1/1     Running             2 (23m ago)     45m   192.168.10.141   centos-node01   <none>           <none>
kube-system    kube-scheduler-centos-master            1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
```

#### 故障排查

查看POD的描述信息

```shell
[root@centos-master ~]# kubectl describe pod -n kube-flannel kube-flannel-ds-bq6hk
---
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  13m                   default-scheduler  Successfully assigned kube-flannel/kube-flannel-ds-bq6hk to centos-node01
  Normal   Pulled     13m                   kubelet            Container image "docker.io/flannel/flannel-cni-plugin:v1.2.0" already present on machine
  Normal   Created    13m                   kubelet            Created container install-cni-plugin
  Normal   Started    13m                   kubelet            Started container install-cni-plugin
  Normal   Pulled     13m                   kubelet            Container image "docker.io/flannel/flannel:v0.22.2" already present on machine
  Normal   Created    13m                   kubelet            Created container install-cni
  Normal   Started    13m                   kubelet            Started container install-cni
  Normal   Pulled     13m (x4 over 13m)     kubelet            Container image "docker.io/flannel/flannel:v0.22.2" already present on machine
  Normal   Created    13m (x4 over 13m)     kubelet            Created container kube-flannel
  Normal   Started    13m (x4 over 13m)     kubelet            Started container kube-flannel
  Warning  BackOff    3m41s (x47 over 13m)  kubelet            Back-off restarting failed container kube-flannel in pod kube-flannel-ds-bq6hk_kube-flannel(ce487bc6-77d9-47c3-85cb-d8b674ed0a49)

```

查看POD的日志信息

```shell
[root@centos-master ~]# kubectl logs -n kube-flannel kube-flannel-ds-bq6hk
Defaulted container "kube-flannel" out of: kube-flannel, install-cni-plugin (init), install-cni (init)
I0918 07:06:07.259809       1 main.go:212] CLI flags config: {etcdEndpoints:http://127.0.0.1:4001,http://127.0.0.1:2379 etcdPrefix:/coreos.com/network etcdKeyfile: etcdCertfile: etcdCAFile: etcdUsername: etcdPassword: version:false kubeSubnetMgr:true kubeApiUrl: kubeAnnotationPrefix:flannel.alpha.coreos.com kubeConfigFile: iface:[] ifaceRegex:[] ipMasq:true ifaceCanReach: subnetFile:/run/flannel/subnet.env publicIP: publicIPv6: subnetLeaseRenewMargin:60 healthzIP:0.0.0.0 healthzPort:0 iptablesResyncSeconds:5 iptablesForwardRules:true netConfPath:/etc/kube-flannel/net-conf.json setNodeNetworkUnavailable:true useMultiClusterCidr:false}
W0918 07:06:07.259882       1 client_config.go:617] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0918 07:06:07.277157       1 kube.go:145] Waiting 10m0s for node controller to sync
I0918 07:06:07.277218       1 kube.go:489] Starting kube subnet manager
I0918 07:06:08.278091       1 kube.go:152] Node controller sync successful
I0918 07:06:08.278308       1 main.go:232] Created subnet manager: Kubernetes Subnet Manager - centos-node01
I0918 07:06:08.278327       1 main.go:235] Installing signal handlers
I0918 07:06:08.278676       1 main.go:543] Found network config - Backend type: vxlan
I0918 07:06:08.278738       1 match.go:206] Determining IP address of default interface
I0918 07:06:08.280615       1 match.go:259] Using interface with name ens32 and address 192.168.10.141
I0918 07:06:08.280816       1 match.go:281] Defaulting external address to interface address (192.168.10.141)
I0918 07:06:08.280996       1 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
E0918 07:06:08.281602       1 main.go:335] Error registering network: failed to acquire lease: node "centos-node01" pod cidr not assigned   ## 这条是关键，报错pod cidr问题
W0918 07:06:08.282035       1 reflector.go:347] github.com/flannel-io/flannel/pkg/subnet/kube/kube.go:490: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
I0918 07:06:08.282111       1 main.go:523] Stopping shutdownHandler...
```

找到故障信息：`Error registering network: failed to acquire lease: node "centos-node01" pod cidr not assigned`，以此判定是`cidr`问题导致。

#### 解决故障

由于在初始化集群的时候未指定参数：`-pod-network-cidr`造成添加网络插件时报错`Error registering network: failed to acquire lease: node "centos-node01" pod cidr not assigned `，建议重新初始化集群并加上参数：`-pod-network-cidr`

不重新初始化的情况下解决方法如下：

在`/etc/kubernetes/manifests/kube-controller-manager.yaml `中添加`- --allocate-node-cidrs=true`和`- --cluster-cidr=10.244.0.0/16`，然后重启`kubelet`即可。

```shell
[root@centos-master ~]# vim /etc/kubernetes/manifests/kube-controller-manager.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --use-service-account-credentials=true
    - --allocate-node-cidrs=true       #添加
    - --cluster-cidr=10.244.0.0/16     #添加
    image: registry.aliyuncs.com/google_containers/kube-controller-manager:v1.28.2
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-controller-manager
    resources:
      requests:
        cpu: 200m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/pki
      name: etc-pki
      readOnly: true
    - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      name: flexvolume-dir
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /etc/kubernetes/controller-manager.conf
      name: kubeconfig
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/pki
      type: DirectoryOrCreate
    name: etc-pki
  - hostPath:
      path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      type: DirectoryOrCreate
    name: flexvolume-dir
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /etc/kubernetes/controller-manager.conf
      type: FileOrCreate
    name: kubeconfig
status: {}

```

重启`kubelet`

```shell
[root@centos-master ~]# systemctl restart kubelet
```



配置文件

```shell
#kmaster1上创建k8s-init-config.yaml
 
vim k8s-init-config.yaml
 
-----------------------整个文件内容--------------------------------
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: wgs001.com3yjucgqr276rf # 可以自定义，正则([a-z0-9]{6}).([a-z0-9]{16})
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.48.210 # 修改成节点ip
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  imagePullPolicy: IfNotPresent
  name: kmaster1 # 节点的hostname
  taints: 
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
---
apiServer:
  timeoutForControlPlane: 4m0s
  certSANs: # 3主个节点IP和vip的ip
  - 192.168.48.210
  - 192.168.48.211
  - 192.168.48.212
  - 192.168.48.222
apiVersion: kubeadm.k8s.io/v1beta3
controlPlaneEndpoint: "192.168.48.222:6443" # 设置vip高可用地址
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers # 设置国内源
kind: ClusterConfiguration
kubernetesVersion: v1.28.0 # 指定版本
networking:
  dnsDomain: k8s.local
  podSubnet: 10.244.0.0/16 # 增加指定pod的网段
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
# 用于配置kube-proxy上为Service指定的代理模式: ipvs or iptables
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "ipvs"
---
# 指定cgroup
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
cgroupDriver: "systemd"
```



# 初始化失败案例

初始化失败会有以下提示内容

```shell
Unfortunately, an error has occurred:
        context deadline exceeded

This error is likely caused by:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
        - 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause'
        Once you have found the failing container, you can inspect its logs with:
        - 'crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs CONTAINERID'
error execution phase wait-control-plane: could not initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher
```

排查问题步骤

查看组件Pod是否启动成功

```shell
root@ubuntu-master:/home/ubuntu# crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a | grep kube | grep -v pause
b34996dfd292c       4db5a05c271ea       About a minute ago   Exited              kube-apiserver            4                   0796af96fc78b       kube-apiserver-ubuntu-master
9914cd4b1bf2c       de1025c2d4968       4 minutes ago        Running             kube-controller-manager   0                   3267619c7a31d       kube-controller-manager-ubuntu-master
d9610513cba65       11492f0faf138       4 minutes ago        Running             kube-scheduler            0                   6fb44c4016da3       kube-scheduler-ubuntu-master
```

以上可以看出组件`kube-apiserver`启动失败，我们查看它的日志，发现有报错

```shell
root@ubuntu-master:/home/ubuntu# crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs b34996dfd292c
I0401 03:14:10.527138       1 options.go:221] external host was not specified, using 43.163.216.231
I0401 03:14:10.528959       1 server.go:148] Version: v1.30.11
I0401 03:14:10.529104       1 server.go:150] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0401 03:14:10.996750       1 shared_informer.go:313] Waiting for caches to sync for node_authorizer
W0401 03:14:10.998501       1 logging.go:59] [core] [Channel #2 SubChannel #3] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:10.999193       1 logging.go:59] [core] [Channel #1 SubChannel #4] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
I0401 03:14:11.009394       1 shared_informer.go:313] Waiting for caches to sync for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0401 03:14:11.013004       1 plugins.go:157] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0401 03:14:11.013027       1 plugins.go:160] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0401 03:14:11.013220       1 instance.go:299] Using reconciler: lease
W0401 03:14:11.013915       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:11.999039       1 logging.go:59] [core] [Channel #2 SubChannel #3] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:12.000294       1 logging.go:59] [core] [Channel #1 SubChannel #4] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:12.015031       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:13.428509       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:13.578419       1 logging.go:59] [core] [Channel #2 SubChannel #3] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:13.786408       1 logging.go:59] [core] [Channel #1 SubChannel #4] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:15.669581       1 logging.go:59] [core] [Channel #2 SubChannel #3] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:16.127806       1 logging.go:59] [core] [Channel #1 SubChannel #4] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:16.190340       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:19.695837       1 logging.go:59] [core] [Channel #1 SubChannel #4] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:20.246988       1 logging.go:59] [core] [Channel #2 SubChannel #3] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:20.558607       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:25.702386       1 logging.go:59] [core] [Channel #1 SubChannel #4] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:25.851397       1 logging.go:59] [core] [Channel #2 SubChannel #3] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0401 03:14:27.585566       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
F0401 03:14:31.014342       1 instance.go:292] Error creating leases: error creating storage factory: context deadline exceeded
```

确认报错日志为

```shell
addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
```

此报错为无法监听链接和监听`etcd`,我们需要查看组件`etcd`是否启动成功

```shell
oot@ubuntu-master:/home/ubuntu# crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock ps -a
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
d45fe1962bcda       2e96e5913fc06       About a minute ago   Exited              etcd                      6                   d0a66bd0571c7       etcd-ubuntu-master
7c95b7e529cce       4db5a05c271ea       2 minutes ago        Exited              kube-apiserver            5                   0796af96fc78b       kube-apiserver-ubuntu-master
9914cd4b1bf2c       de1025c2d4968       6 minutes ago        Running             kube-controller-manager   0                   3267619c7a31d       kube-controller-manager-ubuntu-master
d9610513cba65       11492f0faf138       7 minutes ago        Running             kube-scheduler            0                   6fb44c4016da3       kube-scheduler-ubuntu-master
```

发现etcd启动失败，我们查看日志

```shell
root@ubuntu-master:/home/ubuntu# crictl --runtime-endpoint unix:///var/run/cri-dockerd.sock logs d45fe1962bcda
{"level":"warn","ts":"2025-04-01T03:17:07.468998Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-04-01T03:17:07.469090Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://43.163.216.231:2379","--cert-file=/etc/kubernetes/pki/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://43.163.216.231:2380","--initial-cluster=ubuntu-master=https://43.163.216.231:2380","--key-file=/etc/kubernetes/pki/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://43.163.216.231:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://43.163.216.231:2380","--name=ubuntu-master","--peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/etc/kubernetes/pki/etcd/peer.key","--peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt","--snapshot-count=10000","--trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt"]}
{"level":"warn","ts":"2025-04-01T03:17:07.469184Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-04-01T03:17:07.469195Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://43.163.216.231:2380"]}
{"level":"info","ts":"2025-04-01T03:17:07.469234Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /etc/kubernetes/pki/etcd/peer.crt, key = /etc/kubernetes/pki/etcd/peer.key, client-cert=, client-key=, trusted-ca = /etc/kubernetes/pki/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"error","ts":"2025-04-01T03:17:07.469465Z","caller":"embed/etcd.go:538","msg":"creating peer listener failed","error":"listen tcp 43.163.216.231:2380: bind: cannot assign requested address","stacktrace":"go.etcd.io/etcd/server/v3/embed.configurePeerListeners\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:538\ngo.etcd.io/etcd/server/v3/embed.StartEtcd\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:132\ngo.etcd.io/etcd/server/v3/etcdmain.startEtcd\n\tgo.etcd.io/etcd/server/v3/etcdmain/etcd.go:228\ngo.etcd.io/etcd/server/v3/etcdmain.startEtcdOrProxyV2\n\tgo.etcd.io/etcd/server/v3/etcdmain/etcd.go:135\ngo.etcd.io/etcd/server/v3/etcdmain.Main\n\tgo.etcd.io/etcd/server/v3/etcdmain/main.go:40\nmain.main\n\tgo.etcd.io/etcd/server/v3/main.go:31\nruntime.main\n\truntime/proc.go:267"}
{"level":"info","ts":"2025-04-01T03:17:07.469525Z","caller":"embed/etcd.go:377","msg":"closing etcd server","name":"ubuntu-master","data-dir":"/var/lib/etcd","advertise-peer-urls":["https://43.163.216.231:2380"],"advertise-client-urls":["https://43.163.216.231:2379"]}
{"level":"info","ts":"2025-04-01T03:17:07.469534Z","caller":"embed/etcd.go:379","msg":"closed etcd server","name":"ubuntu-master","data-dir":"/var/lib/etcd","advertise-peer-urls":["https://43.163.216.231:2380"],"advertise-client-urls":["https://43.163.216.231:2379"]}
{"level":"warn","ts":"2025-04-01T03:17:07.469547Z","caller":"etcdmain/etcd.go:146","msg":"failed to start etcd","error":"listen tcp 43.163.216.231:2380: bind: cannot assign requested address"}
{"level":"fatal","ts":"2025-04-01T03:17:07.469579Z","caller":"etcdmain/etcd.go:204","msg":"discovery failed","error":"listen tcp 43.163.216.231:2380: bind: cannot assign requested address","stacktrace":"go.etcd.io/etcd/server/v3/etcdmain.startEtcdOrProxyV2\n\tgo.etcd.io/etcd/server/v3/etcdmain/etcd.go:204\ngo.etcd.io/etcd/server/v3/etcdmain.Main\n\tgo.etcd.io/etcd/server/v3/etcdmain/main.go:40\nmain.main\n\tgo.etcd.io/etcd/server/v3/main.go:31\nruntime.main\n\truntime/proc.go:267"}
```

发现故障日志提示 etcd 无法绑定到 `43.163.216.231:2380`

```shell
{"level":"warn","ts":"2025-04-01T03:17:07.469547Z","caller":"etcdmain/etcd.go:146","msg":"failed to start etcd","error":"listen tcp 43.163.216.231:2380: bind: cannot assign requested address"}
```

你的 `kubeadm init` 失败，日志提示 etcd 无法绑定到 `43.163.216.231:2380`，可能的原因如下：

### 可能原因和解决方案：

1. **IP 地址不可用**

- 你的 `43.163.216.231` 可能不是本机可用的 IP 地址。

- 运行 `ip a` 检查该 IP 是否在你的网卡上绑定：

  ```sh
  ip a | grep 43.163.216.231
  ```

- 如果没有此 IP，你需要使用实际的网卡 IP，或者 `0.0.0.0`（监听所有地址）。

2. **端口冲突**

- 运行以下命令，检查 2380 端口是否被占用：

  ```sh
  netstat -tulnp | grep 2380
  ```

- 如果有其他进程占用，请关闭对应进程：

  ```sh
  sudo kill -9 <PID>
  ```

3. **etcd 配置问题**

- 检查 `/etc/kubernetes/manifests/etcd.yaml` 配置：

  ```sh
  cat /etc/kubernetes/manifests/etcd.yaml | grep advertise-client-urls
  ```

- 确保 `advertise-client-urls` 指定的是可用的 IP，比如：

  ```yaml
  - --advertise-client-urls=https://127.0.0.1:2379
  - --initial-advertise-peer-urls=https://127.0.0.1:2380
  ```

4. **防火墙或 SELinux 限制**

- 关闭防火墙：

  ```sh
  sudo systemctl stop firewalld
  sudo systemctl disable firewalld
  ```

- 关闭 SELinux：

  ```sh
  sudo setenforce 0
  sed -i 's/^SELINUX=enforcing/SELINUX=permissive/' /etc/selinux/config
  ```

5. **清理 kubeadm 重新初始化**

- 如果仍然不行，可以尝试清理并重新初始化：

  ```sh
  kubeadm reset -f
  rm -rf /etc/kubernetes /var/lib/etcd
  kubeadm init
  ```

如果尝试以上方法仍未解决，提供 `kubectl logs -n kube-system etcd-<node-name>` 输出，
