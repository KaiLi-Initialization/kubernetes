# 环境初始化

## 1 检查操作系统的版本

```powershell
# 此方式下安装kubernetes集群系统版本为：CentOS Linux release 7.9.2009 (Core) 版本号：4.18.0-408.el8.x86_64
[root@CentOS-K8S-Master ~]# cat /etc/redhat-release 
CentOS Linux release 7.9.2009 (Core)
[root@centos-master ~]# uname -a
Linux centos-master 3.10.0-1160.95.1.el7.x86_64 #1 SMP Mon Jul 24 13:59:37 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux


```

## 2 主机名解析

为了方便集群节点间的直接调用，在这个配置一下主机名解析，企业中推荐使用内部DNS服务器

```powershell
修改主机名使用：hostnamectl set-hostname 主机名 

# 主机名成解析 编辑三台服务器的/etc/hosts文件，添加下面内容

cat >> /etc/hosts <<EOF
192.168.10.201  master01
192.168.10.202  master02
192.168.10.203  master03
EOF
```

## 3 时间同步

kubernetes要求集群中的节点时间必须精确一直，这里使用chronyd服务从网络同步时间

企业中建议配置内部的会见同步服务器

```powershell
# 启动chronyd服务
[root@master ~]# systemctl start chronyd
[root@master ~]# systemctl enable chronyd
[root@master ~]# date
```

## 4  禁用iptable和firewalld服务

kubernetes和docker 在运行的中会产生大量的iptables规则，为了不让系统规则跟它们混淆，直接关闭系统的规则

```powershell
# 1 关闭firewalld服务
[root@master ~]# systemctl stop firewalld
[root@master ~]# systemctl disable firewalld
# 2 关闭iptables服务
[root@master ~]# systemctl stop iptables
[root@master ~]# systemctl disable iptables
```

## 5 禁用selinux

selinux是linux系统下的一个安全服务，如果不关闭它，在安装集群中会产生各种各样的奇葩问题

```powershell
# 编辑 /etc/selinux/config 文件，修改SELINUX的值为disable
# 注意修改完毕之后需要重启linux服务
SELINUX=disabled

命令方式修改
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
```

## 6 禁用swap分区

swap分区指的是虚拟内存分区，它的作用是物理内存使用完，之后将磁盘空间虚拟成内存来使用，启用swap设备会对系统的性能产生非常负面的影响，因此kubernetes要求每个节点都要禁用swap设备，但是如果因为某些原因确实不能关闭swap分区，就需要在集群安装过程中通过明确的参数进行配置说明

```powershell
方式一：重新启动电脑，永久禁用Swap
# 编辑分区配置文件/etc/fstab，注释掉swap分区一行
# 注意修改完毕之后需要重启linux服务
vim /etc/fstab
注释掉 /dev/mapper/centos-swap swap
# /dev/mapper/centos-swap swap

方式二：不重启电脑，禁用启用swap，立刻生效
swapoff -a 禁用命令
swapon -a  启用命令
free -mh   查看交换分区的状态:
```



## 7 修改linux的内核参数

管理员可以通过 sysctl 接口修改内核运行时的参数,在 `/proc/sys/` 虚拟文件系统下存放许多内核参数。这些参数涉及了多个内核子系统，如：

- 内核子系统（通常前缀为: `kernel.`）
- 网络子系统（通常前缀为: `net.`）
- 虚拟内存子系统（通常前缀为: `vm.`）

若要获取完整的参数列表，请执行以下命令：

```shell
sudo sysctl -a
```

* net.bridge.bridge-nf-call-iptables：开启桥设备内核监控（ipv4）
* net.ipv4.ip_forward：开启路由转发
* net.bridge.bridge-nf-call-ip6tables：开启桥设备内核监控（ipv6）



```shell
内核低于 4.1 版本需要添加 fs.may_detach_mounts=1 和 net.ipv4.tcp_tw_recycle=0

在内核低于 4.1 中，不要设置 net.ipv4.tcp_tw_recycle 这个参数为 1
，网上有不少教程没提到或者内核版本过低系统默认设置为 1
开启此参数，对于外网的 sockets 链接会快速回收。但是对于内网会导致大量的 TCP 链接建立错误。k8s
使用的都是在内网，所以要禁用！设置为 0
有关 net.ipv4.tcp_tw_recycle 参数查看文章(https://cloud.tencent.com/developer/article/1683704)

参数 fs.may_detach_mounts 则是跟容器相关的。该参数如果设置为 0，会导致服务变更后旧 pod
在回收时会一直卡在 Terminating 的状态，会重复出现 UnmountVolume.TearDown failed for volume 错误。
有关 fs.may_detach_mounts 参数产生的 bug 查看文章(https://github.com/kubernetes/kubernetes/issues/51835)
以及 (https://bugzilla.redhat.com/show_bug.cgi?id=1441737) 。
```

以上3项为必须参数，其他参数可根据需要添加

```powershell
# 修改linux的内核采纳数，添加网桥过滤和地址转发功能
# 编辑/etc/sysctl.d/kubernetes.conf文件，添加如下配置：

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# 设置所需的 sysctl 参数，参数在重新启动后保持不变

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# 应用 sysctl 参数而不重新启动
sudo sysctl --system

# 重新加载配置
[root@master ~]# sysctl -p
# 加载网桥过滤模块
[root@master ~]# modprobe br_netfilter
[root@master ~]# modprobe overlay
# 查看网桥过滤模块是否加载成功
[root@master ~]# lsmod | grep br_netfilter
[root@master ~]# lsmod | grep overlay
```

通过运行以下指令确认 `net.bridge.bridge-nf-call-iptables`、`net.bridge.bridge-nf-call-ip6tables` 和 `net.ipv4.ip_forward` 系统变量在你的 `sysctl` 配置中被设置为 1：

```bash
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward
```

## 8 配置ipvs功能

在Kubernetes中Service有两种带来模型，一种是基于iptables的，一种是基于ipvs的两者比较的话，ipvs的性能明显要高一些，但是如果要使用它，需要手动载入ipvs模块

参考文档：https://kubernetes.io/zh-cn/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/

服务代理：https://kubernetes.io/zh-cn/docs/reference/networking/virtual-ips/



IPVS (IP Virtual Server)用于实现传输层负载均衡。

```powershell
# 1.安装ipset和ipvsadm
[root@master ~]# yum install ipset ipvsadm -y
# 2.添加需要加载的模块写入脚本文件
[root@master ~]# cat <<EOF> /etc/sysconfig/modules/ipvs.modules
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
# 在内核4.19+版本nf_conntrack_ipv4已经改为nf_conntrack， 4.18以下使用nf_conntrack_ipv4即可
modprobe -- nf_conntrack_ipv4
EOF
# 3.为脚本添加执行权限
[root@master ~]# chmod +x /etc/sysconfig/modules/ipvs.modules
# 4.执行脚本文件
[root@master ~]# /bin/bash /etc/sysconfig/modules/ipvs.modules
# 5.查看对应的模块是否加载成功
[root@master ~]# lsmod | grep -e ip_vs -e nf_conntrack
低版本运行 ：
[root@master ~]# lsmod | grep -e ip_vs -e nf_conntrack_ipv4
```

## 9 设置系统时区与同步

设置时区

```shell
timedatectl set-timezone Asia/ShanghaiCOPY
```

同步时区

```shell
systemctl enable chronyd && \
systemctl start chronyd
```

查看

```shell
timedatectl status
```

*显示如下*

>`Time zone: Asia/Shanghai (CST, +0800)` 表示使用东八区时区
>`System clock synchronized: yes` 表示时区有同步
>`NTP service: active` 表示开启了时区同步服务

```text
               Local time: 三 2022-09-21 01:12:09 CST
           Universal time: 二 2022-09-20 17:12:09 UTC
                 RTC time: 二 2022-09-20 17:12:09
                Time zone: Asia/Shanghai (CST, +0800)
System clock synchronized: yes
              NTP service: active
          RTC in local TZ: no
```

将当前的 UTC 时间写入硬件时钟

```shell
timedatectl set-local-rtc 0
```

重启依赖于系统时间的服务

```shell
systemctl restart rsyslog && \
systemctl restart crond
```





# 二进制安装containerd

containerd已包含Kubernetes CRI 功能，和docker不同的是无需下载`cri-containerd-....`档案即可使用 CRI。

二进制安装containerd需要**`runc`**和**`CNI`**插件的支持。

#### 第 1 步：安装containerd

所有设备安装（`master`和`node`节点）

containerd 的官方二次制作版本可用于`amd64`（也称为`x86_64`）和`arm64`（也称为`aarch64`）结构。

[从https://github.com/containerd/containerd/releases](https://github.com/containerd/containerd/releases)下载`containerd-<VERSION>-<OS>-<ARCH>.tar.gz`存档，验证其sha256sum，并将其解压到：`/usr/local`

```shell
[root@master ~]# wget https://github.com/containerd/containerd/releases/download/v1.7.5/containerd-1.7.5-linux-amd64.tar.gz

[root@master ~]# tar Cxzvf /usr/local containerd-1.7.5-linux-amd64.tar.gz
bin/
bin/ctr
bin/containerd-stress
bin/containerd
bin/containerd-shim
bin/containerd-shim-runc-v1
bin/containerd-shim-runc-v2


# 文件说明
containerd 的安装包中一共有五个文件，通过上面的命令它们被安装到了 usr/local/bin 目录中：

1. containerd：即容器的运行时，以 gRPC 协议的形式提供满足 OCI 标准的 API

2. containerd-release：containerd 项目的发行版发布工具

3. containerd-stress：containerd压力测试工具

4. containerd-shim：这是每一个容器的运行时载体，我们在 docker 宿主机上看到的 shim 也正是代表着一个个通过调用 containerd 启动的 docker 容器。

5. ctr：它是一个简单的 CLI 接口，用作 containerd 本身的一些调试用途，投入生产使用时还是应该配合docker 或者 cri-containerd 部署。
```

##### 1）修改containerd配置

配置Containerd所需的模块（所有节点）

```shell
cat <<EOF> /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

# 所有节点加载模块：
modprobe -- overlay
modprobe -- br_netfilter
```

创建初始配置文件

Containerd 的默认配置文件为 `/etc/containerd/config.toml`

```shell
$ mkdir -p /etc/containerd/
$ containerd config default > /etc/containerd/config.toml    #创建默认的配置文件
```



##### 2）配置 `systemd` cgroup 驱动

结合 `runc` 使用 `systemd` cgroup 驱动，在 `/etc/containerd/config.toml` 中设置：

```shell
vim /etc/containerd/config.toml

# 找到containerd.runtimes.runc.options，添加SystemdCgroup = true

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes]

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          base_runtime_spec = ""
          cni_conf_dir = ""
          cni_max_conf_num = 0
          container_annotations = []
          pod_annotations = []
          privileged_without_host_devices = false
          runtime_engine = ""
          runtime_path = ""
          runtime_root = ""
          runtime_type = "io.containerd.runc.v2"

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = "true"    # 此处修改为true
            SystemdCgroup = false
```

或者运行以下命令

```shell
sed -i 's#SystemdCgroup = false#SystemdCgroup = true#g' /etc/containerd/config.toml
```



**替换镜像源**

```shell
# 所有节点将sandbox_image的Pause镜像改成符合自己版本的地址：registry.aliyuncs.com/google_containers/pause:3.9
$ vim /etc/containerd/config.toml

  [plugins."io.containerd.grpc.v1.cri"]
    device_ownership_from_security_context = false
    disable_apparmor = false
    disable_cgroup = false
    disable_hugetlb_controller = true
    disable_proc_mount = false
    disable_tcp_service = true
    enable_selinux = false
    enable_tls_streaming = false
    enable_unprivileged_icmp = false
    enable_unprivileged_ports = false
    ignore_image_defined_volumes = false
    max_concurrent_downloads = 3
    max_container_log_line_size = 16384
    netns_mounts_under_state_dir = false
    restrict_oom_score_adj = false
    sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.9" # 此处更换
    
#等同于：
$ sed -i "s#k8s.gcr.io/pause#registry.aliyuncs.com/google_containers/pause#g"  /etc/containerd/config.toml
```



##### 3）通过 systemd 启动 containerd

还应该从https://raw.githubusercontent.com/containerd/containerd/main/containerd.service下载 `containerd.service`文件到目录`/usr/local/lib/systemd/system/containerd.service`(目录/usr/local/lib/systemd/system/需要创建)或`/usr/lib/systemd/system/containerd.service`中，并运行以下命令：

```shell
wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service
mkdir -p /usr/local/lib/systemd/system/
cp containerd.service /usr/local/lib/systemd/system/
```

若无法下载可创建containerd.service将下文复制进去

```shell
vim /usr/lib/systemd/system/containerd.service

# Copyright The containerd Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target local-fs.target

[Service]
#uncomment to fallback to legacy CRI plugin implementation with podsandbox support.
#Environment="DISABLE_CRI_SANDBOXES=1"
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/containerd

Type=notify
Delegate=yes
KillMode=process
Restart=always
RestartSec=5
# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNPROC=infinity
LimitCORE=infinity
LimitNOFILE=infinity
# Comment TasksMax if your systemd version does not supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
OOMScoreAdjust=-999

[Install]
WantedBy=multi-user.target
```

设置开机启动

```shell
$ systemctl daemon-reload
$ systemctl enable --now containerd
$ systemctl status containerd.service
$ containerd  --version    #查看版本
```



#### 第 2 步：安装 runc

[从https://github.com/opencontainers/runc/releases](https://github.com/opencontainers/runc/releases)下载`runc.<ARCH>`二进制文件，验证其 sha256sum，

并将其安装到目录.`/usr/local/sbin/runc`

此处安装版本为：`v1.1.4/runc.amd64`

```shell
[root@master ~]# wget https://github.com/opencontainers/runc/releases/download/v1.1.4/runc.amd64


[root@master ~]# install -m 755 runc.amd64 /usr/local/sbin/runc
#出现以下说明安装没有问题
[root@master ~]# runc -version
runc version 1.1.4
commit: v1.1.4-0-g5fd4c4d1
spec: 1.0.2-dev
go: go1.17.10
libseccomp: 2.5.4

```

该二进制文件是静态构建的，应该适用于任何 Linux 发行版。

#### 第 3 步：安装CNI插件

[从https://github.com/containernetworking/plugins/releases](https://github.com/containernetworking/plugins/releases)下载`cni-plugins-<OS>-<ARCH>-<VERSION>.tgz`存档，验证其 sha256sum，并将其解压到：`/opt/cni/bin`

```shell
[root@master ~]# wget https://github.com/containernetworking/plugins/releases/download/v1.3.0/cni-plugins-linux-amd64-v1.3.0.tgz
[root@master ~]# mkdir -p /opt/cni/bin
[root@master ~]# tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.3.0.tgz
./
./loopback
./bandwidth
./ptp
./vlan
./host-device
./tuning
./vrf
./sbr
./tap
./dhcp
./static
./firewall
./macvlan
./dummy
./bridge
./ipvlan
./portmap
./host-local
```

# 安装Kubernetes组件

此次安装kubernetes版本为`v1.28.1`

```shell
[root@centos-master ~]# kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.1", GitCommit:"8dc49c4b984b897d423aab4971090e1879eb4f23", GitTreeState:"clean", BuildDate:"2023-08-24T11:21:51Z", GoVersion:"go1.20.7", Compiler:"gc", Platform:"linux/amd64"}

```

repo仓库设置

```powershell
# 1、由于kubernetes的镜像在国外，速度比较慢，这里切换成国内的镜像源
# 2、编辑/etc/yum.repos.d/kubernetes.repo,添加下面的配置

# 阿里源

cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgchech=0
repo_gpgcheck=0
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
	   http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 华为源

cat <<EOF > /etc/yum.repos.d/kubernetes.repo 
[kubernetes] 
name=Kubernetes 
baseurl=https://repo.huaweicloud.com/kubernetes/yum/repos/kubernetes-el7-$basearch 
enabled=1 
gpgcheck=1 
repo_gpgcheck=0
gpgkey=https://repo.huaweicloud.com/kubernetes/yum/doc/yum-key.gpg https://repo.huaweicloud.com/kubernetes/yum/doc/rpm-package-key.gpg 
EOF

官方源

cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
enabled=1
gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF

# 3、安装kubeadm、kubelet和kubectl
指定版本
[root@master ~]# yum install --setopt=obsoletes=0 kubeadm-1.17.4-0 kubelet-1.17.4-0 kubectl-1.17.4-0 -y

下载最新版本：
[root@master ~]# yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

# 4、配置kubelet的cgroup
#编辑/etc/sysconfig/kubelet, 添加下面的配置

cat <<EOF | sudo tee /etc/sysconfig/kubelet
KUBELET_CGROUP_ARGS="--cgroup-driver=systemd"
KUBE_PROXY_MODE="ipvs"
EOF

# 5、设置kubelet开机自启
[root@master ~]# systemctl enable kubelet

kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环。
```



# 集群初始化

### 生成kubeadm初始化配置文件

```shell
[root@master ~]# kubeadm config print init-defaults
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 1.2.3.4           # 此处修改为master节点的IP地址
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock   # 此处指定使用的container runtime的sock路径
  imagePullPolicy: IfNotPresent
  name: node
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.k8s.io      # 此处为下载镜像的仓库地址
kind: ClusterConfiguration
kubernetesVersion: 1.26.0       # 修改为所需要的K8S版本号
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
scheduler: {}

# 或者生成配置文件存储在文件：kubeadm.yaml中
[root@master ~]#  kubeadm config print init-defaults > kubeadm.yaml
```

###### 下载集群所需组件镜像

```shell
# 查看所需要的镜像
[root@master ~]# kubeadm config images list
registry.k8s.io/kube-apiserver:v1.28.1
registry.k8s.io/kube-controller-manager:v1.28.1
registry.k8s.io/kube-scheduler:v1.28.1
registry.k8s.io/kube-proxy:v1.28.1
registry.k8s.io/pause:3.9
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1

```

通过配置文件`kubeadm.yaml`下载kubenetes所需镜像

```shell
[root@centos-master ~]# kubeadm config images pull --config /root/kubeadm.yaml

[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.28.1
[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.9
[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.5.9-0
[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.10.1
```

通过配置文件`kubeadm.yaml`初始化集群

```shell
[root@centos-master ~]# kubeadm init --config /root/kubeadm.yaml  --upload-certs

[init] Using Kubernetes version: v1.28.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [centos-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.140]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [centos-master localhost] and IPs [192.168.10.140 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [centos-master localhost] and IPs [192.168.10.140 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 4.526578 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
48f6cf7d44e32a6b4193e0ee36e0301ef95132c4067573cf9b188eeb768d292d
[mark-control-plane] Marking the node centos-master as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node centos-master as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: abcdef.0123456789abcdef
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

#  master节点创建必要文件

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

#  master节点运行

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

#  node节点运行

kubeadm join 192.168.10.140:6443 --token abcdef.0123456789abcdef \
	--discovery-token-ca-cert-hash sha256:be67a796155229a71574a664ec621ac864a3e2abb05e95ca644669692be44f68
```



重置再初始化

```shell
kubeadm reset 

rm -fr ~/.kube/  /etc/kubernetes/* var/lib/etcd/*
```



> 下面的操作只需要在node节点上执行即可（node节点加入集群）

```powershell
# 根据提示把worker节点加进master节点，复制你们各自在日志里的提示，然后分别粘贴在2个worker节点上，最后回车即可（注意要在后面加上--cri-socket unix:///var/run/cri-dockerd.sock这一参数，不然可能会失败）

以node01节点为例：
[root@node01 ~]# kubeadm join 192.168.10.130:6443 --token oat3ii.jimyx3sob5cpi15v --discovery-token-ca-cert-hash sha256:fa442bdfc24302c2e355a2e2462a5972db5492f73312c77169d11805941b88df --cri-socket unix:///var/run/cri-dockerd.sock
[preflight] Running pre-flight checks
	[WARNING FileExisting-tc]: tc not found in system path
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```

在master上查看节点信息

```powershell
[root@master ~]# kubectl get nodes
NAME    STATUS   ROLES     AGE   VERSION
master  NotReady  master   6m    v1.17.4
node1   NotReady   <none>  22s   v1.17.4
node2   NotReady   <none>  19s   v1.17.4
```

##### 安装网络插件，只在master节点操作即可

网络插件`flannel`地址：https://github.com/flannel-io

```shell
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
# 或者
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
```



或者使用网络插件`calico`：https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart

```shell
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml
```

等待它安装完毕稍等片刻， 发现集群的状态已经是Ready

### 网络插件排错

#### 故障现象

网络插件的POD无法创建：

```shell
[root@centos-master ~]# kubectl get pod -A -owide
NAMESPACE      NAME                                    READY   STATUS              RESTARTS        AGE   IP               NODE            NOMINATED NODE   READINESS GATES
kube-flannel   kube-flannel-ds-4fdp8                   0/1     CrashLoopBackOff    7 (2m29s ago)   13m   192.168.10.142   centos-node02   <none>           <none>
kube-flannel   kube-flannel-ds-5wtnh                   0/1     CrashLoopBackOff    7 (2m8s ago)    13m   192.168.10.140   centos-master   <none>           <none>
kube-flannel   kube-flannel-ds-bq6hk                   0/1     CrashLoopBackOff    7 (2m2s ago)    13m   192.168.10.141   centos-node01   <none>           <none>
kube-system    coredns-66f779496c-6qfh9                0/1     ContainerCreating   0               45m   <none>           centos-node02   <none>           <none>
kube-system    coredns-66f779496c-wcmn6                0/1     ContainerCreating   0               45m   <none>           centos-node02   <none>           <none>
kube-system    etcd-centos-master                      1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
kube-system    kube-apiserver-centos-master            1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
kube-system    kube-controller-manager-centos-master   1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
kube-system    kube-proxy-26gbn                        1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
kube-system    kube-proxy-kjmfp                        1/1     Running             2 (23m ago)     44m   192.168.10.142   centos-node02   <none>           <none>
kube-system    kube-proxy-n9hr4                        1/1     Running             2 (23m ago)     45m   192.168.10.141   centos-node01   <none>           <none>
kube-system    kube-scheduler-centos-master            1/1     Running             2 (21m ago)     45m   192.168.10.140   centos-master   <none>           <none>
```

#### 故障排查

查看POD的描述信息

```shell
[root@centos-master ~]# kubectl describe pod -n kube-flannel kube-flannel-ds-bq6hk
---
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  13m                   default-scheduler  Successfully assigned kube-flannel/kube-flannel-ds-bq6hk to centos-node01
  Normal   Pulled     13m                   kubelet            Container image "docker.io/flannel/flannel-cni-plugin:v1.2.0" already present on machine
  Normal   Created    13m                   kubelet            Created container install-cni-plugin
  Normal   Started    13m                   kubelet            Started container install-cni-plugin
  Normal   Pulled     13m                   kubelet            Container image "docker.io/flannel/flannel:v0.22.2" already present on machine
  Normal   Created    13m                   kubelet            Created container install-cni
  Normal   Started    13m                   kubelet            Started container install-cni
  Normal   Pulled     13m (x4 over 13m)     kubelet            Container image "docker.io/flannel/flannel:v0.22.2" already present on machine
  Normal   Created    13m (x4 over 13m)     kubelet            Created container kube-flannel
  Normal   Started    13m (x4 over 13m)     kubelet            Started container kube-flannel
  Warning  BackOff    3m41s (x47 over 13m)  kubelet            Back-off restarting failed container kube-flannel in pod kube-flannel-ds-bq6hk_kube-flannel(ce487bc6-77d9-47c3-85cb-d8b674ed0a49)

```

查看POD的日志信息

```shell
[root@centos-master ~]# kubectl logs -n kube-flannel kube-flannel-ds-bq6hk
Defaulted container "kube-flannel" out of: kube-flannel, install-cni-plugin (init), install-cni (init)
I0918 07:06:07.259809       1 main.go:212] CLI flags config: {etcdEndpoints:http://127.0.0.1:4001,http://127.0.0.1:2379 etcdPrefix:/coreos.com/network etcdKeyfile: etcdCertfile: etcdCAFile: etcdUsername: etcdPassword: version:false kubeSubnetMgr:true kubeApiUrl: kubeAnnotationPrefix:flannel.alpha.coreos.com kubeConfigFile: iface:[] ifaceRegex:[] ipMasq:true ifaceCanReach: subnetFile:/run/flannel/subnet.env publicIP: publicIPv6: subnetLeaseRenewMargin:60 healthzIP:0.0.0.0 healthzPort:0 iptablesResyncSeconds:5 iptablesForwardRules:true netConfPath:/etc/kube-flannel/net-conf.json setNodeNetworkUnavailable:true useMultiClusterCidr:false}
W0918 07:06:07.259882       1 client_config.go:617] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0918 07:06:07.277157       1 kube.go:145] Waiting 10m0s for node controller to sync
I0918 07:06:07.277218       1 kube.go:489] Starting kube subnet manager
I0918 07:06:08.278091       1 kube.go:152] Node controller sync successful
I0918 07:06:08.278308       1 main.go:232] Created subnet manager: Kubernetes Subnet Manager - centos-node01
I0918 07:06:08.278327       1 main.go:235] Installing signal handlers
I0918 07:06:08.278676       1 main.go:543] Found network config - Backend type: vxlan
I0918 07:06:08.278738       1 match.go:206] Determining IP address of default interface
I0918 07:06:08.280615       1 match.go:259] Using interface with name ens32 and address 192.168.10.141
I0918 07:06:08.280816       1 match.go:281] Defaulting external address to interface address (192.168.10.141)
I0918 07:06:08.280996       1 vxlan.go:141] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
E0918 07:06:08.281602       1 main.go:335] Error registering network: failed to acquire lease: node "centos-node01" pod cidr not assigned   ## 这条是关键，报错pod cidr问题
W0918 07:06:08.282035       1 reflector.go:347] github.com/flannel-io/flannel/pkg/subnet/kube/kube.go:490: watch of *v1.Node ended with: an error on the server ("unable to decode an event from the watch stream: context canceled") has prevented the request from succeeding
I0918 07:06:08.282111       1 main.go:523] Stopping shutdownHandler...
```

找到故障信息：`Error registering network: failed to acquire lease: node "centos-node01" pod cidr not assigned`，以此判定是`cidr`问题导致。

#### 解决故障

由于在初始化集群的时候未指定参数：`-pod-network-cidr`造成添加网络插件时报错`Error registering network: failed to acquire lease: node "centos-node01" pod cidr not assigned `，建议重新初始化集群并加上参数：`-pod-network-cidr`

不重新初始化的情况下解决方法如下：

在`/etc/kubernetes/manifests/kube-controller-manager.yaml `中添加`- --allocate-node-cidrs=true`和`- --cluster-cidr=10.244.0.0/16`，然后重启`kubelet`即可。

```shell
[root@centos-master ~]# vim /etc/kubernetes/manifests/kube-controller-manager.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
    - --bind-address=127.0.0.1
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --cluster-name=kubernetes
    - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
    - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
    - --controllers=*,bootstrapsigner,tokencleaner
    - --kubeconfig=/etc/kubernetes/controller-manager.conf
    - --leader-elect=true
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --root-ca-file=/etc/kubernetes/pki/ca.crt
    - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
    - --use-service-account-credentials=true
    - --allocate-node-cidrs=true       #添加
    - --cluster-cidr=10.244.0.0/16     #添加
    image: registry.aliyuncs.com/google_containers/kube-controller-manager:v1.28.2
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-controller-manager
    resources:
      requests:
        cpu: 200m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/pki
      name: etc-pki
      readOnly: true
    - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      name: flexvolume-dir
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /etc/kubernetes/controller-manager.conf
      name: kubeconfig
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/pki
      type: DirectoryOrCreate
    name: etc-pki
  - hostPath:
      path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      type: DirectoryOrCreate
    name: flexvolume-dir
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /etc/kubernetes/controller-manager.conf
      type: FileOrCreate
    name: kubeconfig
status: {}

```

重启`kubelet`

```shell
[root@centos-master ~]# systemctl restart kubelet
```





